{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38295090-a540-40f0-8114-867142ec5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76cd3c50-9c6b-447f-9e93-90fb5cd49f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"RAVEN-10000-release.zip\"\n",
    "#must delete dataset file if want to redo\n",
    "if not os.path.isdir('RAVEN-10000'):\n",
    "    with zipfile.ZipFile(path,mode='r') as dataset:\n",
    "        dataset.extractall('RAVEN-10000') #extract into dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c700b3-05f8-4c88-a2ac-fcd7161e14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing Zhang's utils.py\n",
    "from utils import dataset\n",
    "\n",
    "path = 'RAVEN-10000_dset/' #glob only worked here with relative file path\n",
    "dsetype = 'train'\n",
    "img_size = 160\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "\n",
    "train_set = dataset(dataset_path=path,dataset_type='train',\n",
    "                    img_size=img_size,test=False)\n",
    "#wraps iterable around training dataset\n",
    "train_loader = DataLoader(train_set,batch_size = 32,\n",
    "                          shuffle=True)\n",
    "#returns iterator \n",
    "train_loader_iter = iter(train_loader)\n",
    "images,targets = next(train_loader_iter)\n",
    "\n",
    "#plot imgs\n",
    "if 0: \n",
    "    train_instance =  images[0]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4,4,i+1) #this the important one\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(train_instance[i],cmap=plt.cm.binary)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c97d8b-59e7-475e-a0b6-5406e3a2dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring model from input\n",
    "import networks\n",
    "self = networks.CoPINet()\n",
    "x = images.view(-1,16,80,80)\n",
    "\n",
    "N,_,H,W = x.shape\n",
    "input_features = self.maxpool(\n",
    "    self.relu(\n",
    "        self.bn1(\n",
    "            self.conv1(\n",
    "                x.view(-1,80,80).unsqueeze(1)))))\n",
    "\n",
    "#inference branch\n",
    "\n",
    "prior = x[:,:8,:,:] #first eight imgs (no choices)\n",
    "input_features = self.maxpool(\n",
    "    self.relu(\n",
    "        self.inf_bn1(\n",
    "            self.inf_conv1(prior.contiguous().view(-1,80,80).unsqueeze(1)))))\n",
    "        \n",
    "input_features = input_features.view(-1,8,64,20,20)\n",
    "\n",
    "row1_features = torch.sum(input_features[:, 0:3, :, :, :], dim=1)\n",
    "row2_features = torch.sum(input_features[:, 3:6, :, :, :], dim=1)\n",
    "row_features = self.relu(\n",
    "    self.inf_bn_row(\n",
    "        self.inf_conv_row(\n",
    "            torch.cat((row1_features, row2_features), dim=0))))\n",
    "final_row_features = row_features[:N, :, :, :] + row_features[\n",
    "    N:, :, :, :]\n",
    "\n",
    "col1_features = torch.sum(input_features[:, 0:9:3, :, :, :], dim=1)\n",
    "col2_features = torch.sum(input_features[:, 1:9:3, :, :, :], dim=1)\n",
    "col_features = self.relu(\n",
    "    self.inf_bn_col(\n",
    "        self.inf_conv_col(\n",
    "            torch.cat((col1_features, col2_features), dim=0))))\n",
    "final_col_features = col_features[:N, :, :, :] + col_features[\n",
    "    N:, :, :, :]\n",
    "\n",
    "input_features = final_row_features + final_col_features\n",
    "input_features = self.avgpool(input_features).view(-1, 64)\n",
    "\n",
    "predict_rules = self.predict_rule(\n",
    "    input_features)  # N, self.num_attr * self.num_rule\n",
    "predict_rules = predict_rules.view(-1, self.num_rule)\n",
    "predict_rules = self.inference(predict_rules)\n",
    "\n",
    "basis_bias = self.basis_bias(predict_rules)  # N * self.num_attr, 64\n",
    "basis_bias = torch.sum(basis_bias.view(-1, self.num_attr, 64),\n",
    "                       dim=1)  # N, 64\n",
    "\n",
    "contrast1_bias = self.contrast1_bias_trans(basis_bias)\n",
    "contrast1_bias = contrast1_bias.view(-1, 64, 1,\n",
    "                                     1).expand(-1, -1, 20, 20)\n",
    "contrast2_bias = self.contrast2_bias_trans(basis_bias)\n",
    "contrast2_bias = contrast2_bias.view(-1, 64, 1,\n",
    "                                     1).expand(-1, -1, 10, 10)\n",
    "\n",
    "# Perception Branch\n",
    "input_features = self.maxpool(\n",
    "    self.relu(self.bn1(self.conv1(x.view(-1, 80, 80).unsqueeze(1)))))\n",
    "input_features = input_features.view(-1, 16, 64, 20, 20)\n",
    "\n",
    "choices_features = input_features[:, 8:, :, :, :].unsqueeze(\n",
    "    2)  # N, 8, 64, 20, 20 -> N, 8, 1, 64, 20, 20\n",
    "\n",
    "row1_features = torch.sum(input_features[:, 0:3, :, :, :],\n",
    "                          dim=1)  # N, 64, 20, 20\n",
    "row2_features = torch.sum(input_features[:, 3:6, :, :, :],\n",
    "                          dim=1)  # N, 64, 20, 20\n",
    "row3_pre = input_features[:, 6:8, :, :, :].unsqueeze(1).expand(\n",
    "    N, 8, 2, 64, 20, 20\n",
    ")  # N, 2, 64, 20, 20 -> N, 1, 2, 64, 20, 20 -> N, 8, 2, 64, 20, 20\n",
    "row3_features = torch.sum(\n",
    "    torch.cat((row3_pre, choices_features), dim=2), dim=2).view(\n",
    "        -1, 64, 20, 20\n",
    "    )  # N, 8, 3, 64, 20, 20 -> N, 8, 64, 20, 20 -> N * 8, 64, 20, 20\n",
    "row_features = self.relu(\n",
    "    self.bn_row(\n",
    "        self.conv_row(\n",
    "            torch.cat((row1_features, row2_features, row3_features),\n",
    "                      dim=0))))\n",
    "\n",
    "row1 = row_features[:N, :, :, :].unsqueeze(1).unsqueeze(1).expand(\n",
    "    N, 8, 1, 64, 20, 20)\n",
    "row2 = row_features[N:2 * N, :, :, :].unsqueeze(1).unsqueeze(1).expand(\n",
    "    N, 8, 1, 64, 20, 20)\n",
    "row3 = row_features[2 * N:, :, :, :].view(-1, 8, 64, 20,\n",
    "                                          20).unsqueeze(2)\n",
    "final_row_features = torch.sum(torch.cat((row1, row2, row3), dim=2),\n",
    "                               dim=2)\n",
    "\n",
    "col1_features = torch.sum(input_features[:, 0:9:3, :, :, :],\n",
    "                          dim=1)  # N, 64, 20, 20\n",
    "col2_features = torch.sum(input_features[:, 1:9:3, :, :, :],\n",
    "                          dim=1)  # N, 64, 20, 20\n",
    "col3_pre = input_features[:, 2:8:3, :, :, :].unsqueeze(1).expand(\n",
    "    N, 8, 2, 64, 20, 20\n",
    ")  # N, 2, 64, 20, 20 -> N, 1, 2, 64, 20, 20 -> N, 8, 2, 64, 20, 20\n",
    "col3_features = torch.sum(\n",
    "    torch.cat((col3_pre, choices_features), dim=2), dim=2).view(\n",
    "        -1, 64, 20, 20\n",
    "    )  # N, 8, 3, 64, 20, 20 -> N, 8, 64, 20, 20 -> N * 8, 64, 20, 20\n",
    "col_features = self.relu(\n",
    "    self.bn_col(\n",
    "        self.conv_col(\n",
    "            torch.cat((col1_features, col2_features, col3_features),\n",
    "                      dim=0))))\n",
    "\n",
    "col1 = col_features[:N, :, :, :].unsqueeze(1).unsqueeze(1).expand(\n",
    "    N, 8, 1, 64, 20, 20)\n",
    "col2 = col_features[N:2 * N, :, :, :].unsqueeze(1).unsqueeze(1).expand(\n",
    "    N, 8, 1, 64, 20, 20)\n",
    "col3 = col_features[2 * N:, :, :, :].view(-1, 8, 64, 20,\n",
    "                                          20).unsqueeze(2)\n",
    "final_col_features = torch.sum(torch.cat((col1, col2, col3), dim=2),\n",
    "                               dim=2)\n",
    "\n",
    "input_features = final_row_features + final_col_features\n",
    "input_features = input_features.view(-1, 64, 20, 20)\n",
    "\n",
    "res1_in = input_features.view(-1, 8, 64, 20, 20)\n",
    "res1_contrast = self.res1_contrast_bn(\n",
    "    self.res1_contrast(\n",
    "        torch.cat((torch.sum(res1_in, dim=1), contrast1_bias), dim=1)))\n",
    "res1_in = res1_in - res1_contrast.unsqueeze(1)\n",
    "res2_in = self.res1(res1_in.view(-1, 64, 20, 20))\n",
    "res2_in = res2_in.view(-1, 8, 128, 10, 10)\n",
    "res2_contrast = self.res2_contrast_bn(\n",
    "    self.res2_contrast(\n",
    "        torch.cat((torch.sum(res2_in, dim=1), contrast2_bias), dim=1)))\n",
    "res2_in = res2_in - res2_contrast.unsqueeze(1)\n",
    "out = self.res2(res2_in.view(-1, 128, 10, 10))\n",
    "\n",
    "avgpool = self.avgpool(out)\n",
    "avgpool = avgpool.view(-1, 256)\n",
    "final = avgpool\n",
    "final = self.mlp(final)\n",
    "final = final.view(-1,8)\n",
    "\n",
    "model_output = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a1c1ebc-281e-4248-b93c-b231536f829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import criteria\n",
    "loss,G,zeros = criteria.contrast_loss(model_output,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6040e50a-9ae8-49ef-aafa-5aa98735ed2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notes on network.py structure:\n",
      "\n",
      "- Gumbel-softmax on line 127. So all that stuff before is the encoding? And the stuff\n",
      "after is preparing contrast_bias 1 and 2, for input to perception branch\n",
      "\n",
      "- Creating 'input features' from x: This is the job of the conv layer (and relu and batchnorm).\n",
      "Remember, conv layers take an image and produce a feature map (through application of conv operator \n",
      "then some type of pooling), where the feature map is a score against learned kernel.\n",
      "\n",
      "- Lines 103 onwards collect features across row and column.\n",
      "\n",
      "- It seems like lines 140 through to 201 is just the feature encoding for \n",
      "the perception branch\n",
      "\n",
      "- It looks like 200 - 214 is the contrast block. Takes input features (avgd across rows and columns). \n",
      "Then through convnet, batchnorm, resblock. \n",
      "\n",
      "- Note that there are 2 contrast blocks working in series. Don't know why this is the case.\n",
      "\n",
      "Notes on loss: \n",
      "\n",
      "- So CoPINet returns an (m,8) tensor. \n",
      "    Hypothesis 1 - the eight vectors here are positions of all OUa's in some latent space\n",
      "    Hypothesis 2 - Each of the eight 128 vectors are negative potentials for the 80x80 quarter-images (32 x 4 = 128)\n",
      "    \n",
      "- Use BCE with logits because we're inputting negative potentials (logits) into \n",
      "cost function, NOT probabilities.\n",
      "\n",
      "- G contains potentials (only first 32 rows are relevant). zeros are actual answers (this tensor is created\n",
      "from 'targets')\n",
      "\n",
      "\n",
      "Questions for Dr.:\n",
      "\n",
      "- How are the conv layers designed? The kernel sizes, stride lengths seem pretty arbitrary to me and no mention in paper\n",
      "- First step - reshape (160,160) imgs to (80,80).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Notes on network.py structure:\n",
    "\n",
    "- Gumbel-softmax on line 127. So all that stuff before is the encoding? And the stuff\n",
    "after is preparing contrast_bias 1 and 2, for input to perception branch\n",
    "\n",
    "- Creating 'input features' from x: This is the job of the conv layer (and relu and batchnorm).\n",
    "Remember, conv layers take an image and produce a feature map (through application of conv operator \n",
    "then some type of pooling), where the feature map is a score against learned kernel.\n",
    "\n",
    "- Lines 103 onwards collect features across row and column.\n",
    "\n",
    "- It seems like lines 140 through to 201 is just the feature encoding for \n",
    "the perception branch\n",
    "\n",
    "- It looks like 200 - 214 is the contrast block. Takes input features (avgd across rows and columns). \n",
    "Then through convnet, batchnorm, resblock. \n",
    "\n",
    "- Note that there are 2 contrast blocks working in series. Don't know why this is the case.\n",
    "\n",
    "Notes on loss: \n",
    "\n",
    "- So CoPINet returns an (m,8) tensor. \n",
    "    Hypothesis 1 - the eight vectors here are positions of all OUa's in some latent space\n",
    "    Hypothesis 2 - Each of the eight 128 vectors are negative potentials for the 80x80 quarter-images (32 x 4 = 128)\n",
    "    \n",
    "- Use BCE with logits because we're inputting negative potentials (logits) into \n",
    "cost function, NOT probabilities.\n",
    "\n",
    "- G contains potentials (only first 32 rows are relevant). zeros are actual answers (this tensor is created\n",
    "from 'targets')\n",
    "\n",
    "\n",
    "Questions for Dr.:\n",
    "\n",
    "- How are the conv layers designed? The kernel sizes, stride lengths seem pretty arbitrary to me and no mention in paper\n",
    "- First step - reshape (160,160) imgs to (80,80).\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c028488-62e3-473e-b3f9-4e559c44f1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8913-085c-4a31-9674-5fb4aac137dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fmenv]",
   "language": "python",
   "name": "conda-env-fmenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
