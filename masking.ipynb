{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl;\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import data\n",
    "import utils\n",
    "import sys\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from abc import ABC,abstractmethod\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from copy import deepcopy\n",
    "from torch.special import logit\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "importlib.reload(data)\n",
    "importlib.reload(utils)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "debug=False\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "\n",
    "batch_size=64\n",
    "epochs=2\n",
    "subset_frac=0.1\n",
    "\n",
    "custom_dataset_test_split=0.2\n",
    "\n",
    "data_path='datasets'\n",
    "\n",
    "train_dataset = MNIST(data_path,train=True, transform=transforms.ToTensor())\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "\n",
    "val_dataset=MNIST(data_path,train=False,transform=transforms.ToTensor())\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size)\n",
    "\n",
    "n_task=5\n",
    "n_task_=[1,2,3,4,6,7,8,9]\n",
    "\n",
    "if not debug:\n",
    "    custom1_datamodule=data.CustomDataModule(n=n_task)\n",
    "    custom1_datamodule.setup()\n",
    "    custom1_train_dataloader=custom1_datamodule.train_dataloader()\n",
    "    custom1_test_dataloader=custom1_datamodule.test_dataloader()\n",
    "\n",
    "    custom2_datamodule=data.CustomDataModule(n=n_task_)\n",
    "    custom2_datamodule.setup()\n",
    "    custom2_train_dataloader=custom2_datamodule.train_dataloader()\n",
    "    custom2_test_dataloader=custom2_datamodule.test_dataloader()\n",
    "\n",
    "if debug:\n",
    "    custom1_datamodule=data.CustomDataModule(n=n_task,dataset_frac=subset_frac)\n",
    "    custom1_datamodule.setup()\n",
    "    custom1_train_dataloader=custom1_datamodule.train_dataloader()\n",
    "    custom1_test_dataloader=custom1_datamodule.test_dataloader()\n",
    "\n",
    "    custom2_datamodule=data.CustomDataModule(n=n_task_,dataset_frac=subset_frac)\n",
    "    custom2_datamodule.setup()\n",
    "    custom2_train_dataloader=custom2_datamodule.train_dataloader()\n",
    "    custom2_test_dataloader=custom2_datamodule.test_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "X,y=next(iter(train_dataloader))\n",
    "X_c,y_c=next(iter(custom1_train_dataloader))\n",
    "X_c2,y_c2=next(iter(custom2_train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular MNIST Model\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(28*28,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.l1(x)\n",
    "        return y\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,img_size):\n",
    "        super().__init__()\n",
    "        H,W=img_size\n",
    "        nf1=10\n",
    "        nf2=20\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,nf1,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2 = nn.Conv2d(10,nf2,kernel_size=3,stride=1,padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        lin_size=int(0.25*H*0.25*W*nf2) #must be integer\n",
    "        self.fc1 = nn.Linear(in_features=lin_size,out_features=50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        N=x.size()[0]\n",
    "\n",
    "        x=F.relu(self.maxpool2(self.conv1(x)))\n",
    "        x=F.relu(self.maxpool2(self.conv2_drop(self.conv2(x)))) #dropout in conv layers\n",
    "        x=x.view(N,-1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.dropout(x,training=self.training) #dropout in FFN layers\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        self.log('train-loss',loss.item(),on_epoch=True)\n",
    "        #wandb.log('train/loss':loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        \n",
    "        self.log('val-loss',loss.item(),on_epoch=True)\n",
    "        self.log('val-acc',acc,on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n",
    "\n",
    "class MNISTFFN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Flatten(start_dim=1,end_dim=-1),\n",
    "            nn.Linear(28*28,256),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Linear(128,64),\n",
    "            nn.Linear(64,10),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "\n",
    "        #logging\n",
    "        self.logger.experiment.add_scalars('Pretrained loss',{'train':loss.item()},self.global_step)\n",
    "        self.logger.experiment.add_scalars('Pretrained accuracy',{'train':acc},self.global_step)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        \n",
    "        #logging\n",
    "        self.logger.experiment.add_scalars('Pretrained loss',{'val':loss.item()},self.global_step)\n",
    "        self.logger.experiment.add_scalars('Pretrained accuracy',{'val':acc},self.global_step)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretraining model - SHOULDN't BE RUNNING OFTEN\n",
    "\n",
    "\n",
    "checkpoint_dir='checkpoints'\n",
    "\n",
    "name=str(input('Log name'))\n",
    "wandb_logger=WandbLogger(project='AVR',name=name,version=name)\n",
    "\n",
    "if debug==True:\n",
    "\n",
    "    limit_train_batches=0.05\n",
    "    limit_val_batches=0.01\n",
    "    \n",
    "else:\n",
    "    limit_train_batches=1.0\n",
    "    limit_val_batches=1.0\n",
    "\n",
    "check_val_every=1\n",
    "\n",
    "epochs=10\n",
    "model=MNISTModel(img_size=(28,28))\n",
    "\n",
    "#callbacks\n",
    "ES_callback=EarlyStopping(monitor='val-loss',patience=100)\n",
    "ckpt_callback=ModelCheckpoint(\n",
    "    save_top_k=1, #save top 1 checkpoint,\n",
    "    monitor='val-acc',\n",
    "    mode='max',\n",
    "    filename='max-val-acc-ckpt'\n",
    ")\n",
    "\n",
    "\n",
    "trainer=pl.Trainer(max_epochs=epochs,\n",
    "                    check_val_every_n_epoch=check_val_every,\n",
    "                    limit_train_batches=limit_train_batches,limit_val_batches=limit_val_batches,\n",
    "                    logger=wandb_logger,callbacks=[ES_callback],\n",
    "                    default_root_dir=checkpoint_dir)\n",
    "\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelA=MNISTFFN().load_from_checkpoint('logs/lightning_logs/version_7/checkpoints/epoch=4-step=4690.ckpt')\n",
    "modelB=MNISTModel() #with conv enc layer\n",
    "\n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return model\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "\n",
    "\n",
    "def get_named_children(model):\n",
    "\n",
    "    '''\n",
    "    IMPORTANT: We assume that a leaf child is one that has 0 children\n",
    "    This needs checking\n",
    "    '''\n",
    "    \n",
    "    children_dict={}\n",
    "    named_modules=dict(model.named_modules())\n",
    "    for module_name,module in named_modules.items():\n",
    "        if len(list(module.children()))==0:\n",
    "            children_dict[module_name]=module\n",
    "\n",
    "    return children_dict\n",
    "\n",
    "class MaskedModel():\n",
    "\n",
    "    def __init__(self,model,train_dataloader,test_dataloader1,test_dataloader2,tau=1):\n",
    "\n",
    "        self.model=model\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.test_dataloader1=test_dataloader1\n",
    "        self.test_dataloader2=test_dataloader2\n",
    "\n",
    "        self.logit_tensors_dict={k:torch.nn.Parameter(data=torch.full_like(p,0.9)) for k,p in modelA.named_parameters()}\n",
    "        self.alpha=None\n",
    "        self.tau=tau\n",
    "        self.logging=False\n",
    "\n",
    "        self.train_epoch=0\n",
    "\n",
    "\n",
    "        #freeze model parameters\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad=False\n",
    "        self.param_dict=dict(model.named_parameters())\n",
    "\n",
    "        self.leaf_modules=get_named_children(self.model)\n",
    "\n",
    "\n",
    "        self.optimiser=torch.optim.Adam(self.logit_tensors_dict.values())\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        '''\n",
    "        Forward through masked model \n",
    "        Masked model - Frozen pretrained model * binaries\n",
    "        '''\n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "\n",
    "        #apply mask to (frozen) tensors\n",
    "        for layer_name,layer in self.leaf_modules.items():\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                weight_=self.param_dict[layer_name+'.weight']*binaries[layer_name+'.weight']\n",
    "                bias_=self.param_dict[layer_name+'.bias']*binaries[layer_name+'.bias']\n",
    "                x=F.linear(x,weight=weight_,bias=bias_) #calling next rather than idx-ing ensures tensor not detached from computational graph (what does this mean)\n",
    "            else:\n",
    "                x=layer(x)\n",
    "\n",
    "        return x\n",
    "        #we must implement the forward layer ourself\n",
    "\n",
    "        '''\n",
    "        for layer_name,layer in self.layers:\n",
    "            if linear:\n",
    "                x=F.linear(x,weight=binaries[layer_name+'weight'],bias=binaries[layer_name+'bias'])\n",
    "        '''\n",
    "        \n",
    "    \n",
    "       \n",
    "\n",
    "    def train(self,alpha,n_batches=10,n_epochs=5,logging=False,\n",
    "            val_every_n_steps=10,\n",
    "            eval_every=10,n_eval_batches=5,norm_freq=5,set_log_name=False):\n",
    "\n",
    "        if logging:\n",
    "            self.logging=True\n",
    "        if self.logging:\n",
    "            if set_log_name:\n",
    "                log_name=str(input('Enter log name'))\n",
    "                if log_name=='':\n",
    "                    sys.exit()\n",
    "            else:\n",
    "                log_name=None\n",
    "            run=wandb.init(project='AVR',name=log_name)\n",
    "\n",
    "        for ep in range(n_epochs):\n",
    "\n",
    "            #set class attributes to be used in other methods called in 'train()'\n",
    "            self.train_epoch+=1\n",
    "            self.alpha=alpha\n",
    "\n",
    "\n",
    "            for batch_idx,batch in enumerate(train_dataloader):\n",
    "                if n_batches=='full':\n",
    "                    pass\n",
    "                if batch_idx==n_batches:\n",
    "                    break\n",
    "                x,y=batch\n",
    "                y_hat=self.forward(x)\n",
    "\n",
    "                crossent_loss=F.cross_entropy(y_hat,y)\n",
    "                reg_loss=alpha*torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "                loss=crossent_loss+reg_loss\n",
    "                acc=utils.calculate_accuracy(y_hat,y)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "\n",
    "                if self.logging:\n",
    "                    wandb.log({'epoch':ep,\n",
    "                                'train_loss':loss.item(),\n",
    "                                'train_crossent_loss':crossent_loss.item(),\n",
    "                                'train_reg_loss':reg_loss.item(),\n",
    "                                'train_accuracy':acc\n",
    "                                })\n",
    "                                \n",
    "                    if ep%norm_freq==0 and ep!=0:\n",
    "                        data=self.param_grad_norms()\n",
    "                        table=wandb.Table(data=data,columns=['names','key','values'])\n",
    "                        wandb.log({'Norm data':table})\n",
    "\n",
    "                if (run.step%val_every_n_steps==0) and (run.step!=0):\n",
    "                    self.validation()\n",
    "\n",
    "\n",
    "            #evaluate via ablation and comparison to other tasks\n",
    "            if (ep%eval_every==0):\n",
    "                self.eval(self.test_dataloader1,self.test_dataloader2,n_batches=n_eval_batches)\n",
    "            \n",
    "            print(f'Epoch: {ep}, Loss:{loss.item()}')\n",
    "\n",
    "\n",
    "        \n",
    "        if self.logging:\n",
    "            wandb.finish()\n",
    "\n",
    "    \n",
    "    def transform_logit_tensors(self):\n",
    "\n",
    "        tau=self.tau\n",
    "\n",
    "        U1 = torch.rand(1, requires_grad=True)\n",
    "        U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "        samples={}\n",
    "        for k,v in self.logit_tensors_dict.items():\n",
    "            samples[k]=torch.sigmoid((v - torch.log(torch.log(U1) / torch.log(U2))) / tau)\n",
    "            \n",
    "\n",
    "        binaries_stop={}\n",
    "        for k,v in samples.items():\n",
    "            with torch.no_grad():\n",
    "                binaries_stop[k]=(v>0.5).float()-v\n",
    "        \n",
    "        binaries={}\n",
    "        for k,v in binaries_stop.items():\n",
    "            binaries[k]=v+samples[k]\n",
    "\n",
    "        return binaries\n",
    "\n",
    "\n",
    "    def validation(self):\n",
    "        '''\n",
    "        Calculate accuracy of mask on validation set\n",
    "        For now, we use test_dataset=val_dataset\n",
    "        '''\n",
    "        \n",
    "        batch=next(iter(self.test_dataloader1))\n",
    "        x,y=batch\n",
    "        with torch.no_grad():\n",
    "            y_hat=self.forward(x)\n",
    "\n",
    "        crossent_loss=F.cross_entropy(y_hat,y)\n",
    "        reg_loss=self.alpha*torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "        loss=crossent_loss+reg_loss\n",
    "        acc=utils.calculate_accuracy(y_hat,y)\n",
    "\n",
    "\n",
    "\n",
    "        if self.logging:\n",
    "                wandb.log({\n",
    "                            'validation_loss':loss.item(),\n",
    "                            'validation_crossent_loss':crossent_loss.item(),\n",
    "                            'validation_reg_loss':reg_loss.item(),\n",
    "                            'validation_accuracy':acc\n",
    "                            })\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self,task_eval_dataloader,_task_eval_dataloader,n_batches):\n",
    "        '''\n",
    "        Evaluated mask via ablation\n",
    "\n",
    "        Ablation - frozen_parameters * ~binaries (inverted mask)\n",
    "        '''\n",
    "        print('start eval')\n",
    "        #create masked model\n",
    "        masked_model=copy.deepcopy(self.model)\n",
    "        with torch.no_grad():\n",
    "            binaries=self.transform_logit_tensors()\n",
    "\n",
    "            #only linear layer compatibility at the moment \n",
    "            for n,p in masked_model.named_parameters():\n",
    "                invert_mask=(~(binaries[n].bool())).int() #to ablate TASK weights, we invert mask\n",
    "                masked_param=p*invert_mask\n",
    "                p.copy_(masked_param) #copy in masked params\n",
    "\n",
    "        acc1s=[]\n",
    "        acc2s=[]\n",
    "\n",
    "        for batch_idx,(batch1,batch2) in enumerate(zip(task_eval_dataloader,_task_eval_dataloader)):\n",
    "            if n_batches=='full':\n",
    "                pass\n",
    "            if batch_idx==n_batches:\n",
    "                break\n",
    "            x1,y1=batch1\n",
    "            x2,y2=batch2\n",
    "\n",
    "            pred_logits_1=masked_model(x1)\n",
    "            pred_logits_2=masked_model(x2)\n",
    "\n",
    "            acc1s.append(utils.calculate_accuracy(pred_logits_1,y1))\n",
    "            acc2s.append(utils.calculate_accuracy(pred_logits_2,y2))\n",
    "\n",
    "        acc1=round(np.mean(acc1s),2)\n",
    "        acc2=round(np.mean(acc2s),2)\n",
    "\n",
    "        if self.logging:\n",
    "            wandb.define_metric(\"Eval accuracies\",step_metric='epoch')\n",
    "            wandb.log({'Eval accuracies':{\"Task\":acc1,\"NOT task\":acc2}})\n",
    "        else:\n",
    "            print({\"Acc task'\":acc1,\"Acc not task\":acc2})\n",
    "\n",
    "        print('end eval')\n",
    "\n",
    "    def param_grad_norms(self):\n",
    "        data=[]\n",
    "        names=list(self.logit_tensors_dict.keys())\n",
    "        for name in names:\n",
    "            data.append([name,'param norm',self.logit_tensors_dict[name].mean().item()])\n",
    "            data.append([name,'grad norm',self.logit_tensors_dict[name].grad.mean().item()])\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "\n",
    "\n",
    "kwargs={\n",
    "    'alpha':0.0,\n",
    "    'n_epochs':5,\n",
    "    'n_batches':10,\n",
    "    'val_every_n_steps':10,\n",
    "    'eval_every':1,\n",
    "    'n_eval_batches':1,\n",
    "    'norm_freq':100,\n",
    "    'logging':True,\n",
    "    'set_log_name':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if 1:\n",
    "    mm=MaskedModel(modelA,train_dataloader=task_train_dataloader,\n",
    "        test_dataloader1=task_test_dataloader,test_dataloader2=_task_test_dataloader)\n",
    "    mm.train(**kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMaskedModel(ABC):\n",
    "\n",
    "    def __init__(self,model,train_dataloader,test_dataloader1,test_dataloader2,tau=1):\n",
    "        \n",
    "        self.model=model\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.test_dataloader1=test_dataloader1\n",
    "        self.test_dataloader2=test_dataloader2\n",
    "\n",
    "        self.logit_tensors_dict={k:torch.nn.Parameter(data=torch.full_like(p,0.9)) for k,p in model.named_parameters()}\n",
    "        self.alpha=None\n",
    "        self.tau=tau\n",
    "        self.logging=False\n",
    "\n",
    "\n",
    "\n",
    "        #freeze model parameters\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad=False\n",
    "        self.param_dict=dict(model.named_parameters())\n",
    "\n",
    "        self.leaf_modules=utils.get_named_children(self.model)\n",
    "\n",
    "\n",
    "        self.optimiser=torch.optim.Adam(self.logit_tensors_dict.values())\n",
    "\n",
    "\n",
    "        self.glob_step=0\n",
    "        self.train_epoch=0\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self,x,invert_mask=False):\n",
    "        pass\n",
    "\n",
    "    def calculate_loss(self,y_hat,y):\n",
    "        crossent_loss=F.cross_entropy(y_hat,y)\n",
    "        reg_loss=self.alpha*torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "        loss=crossent_loss+reg_loss\n",
    "        acc=utils.calculate_accuracy(y_hat,y)\n",
    "\n",
    "        return crossent_loss,reg_loss,loss,acc\n",
    "\n",
    "    def train(self,alpha,n_batches=10,n_epochs=5,logging=False,\n",
    "            val_every_n_steps=10,\n",
    "            eval_every=10,n_eval_batches=5,norm_freq=5,set_log_name=False):\n",
    "\n",
    "            #set class attributes for use in rest of class\n",
    "            self.alpha=alpha\n",
    "            \n",
    "            if logging:\n",
    "                self.logging=True\n",
    "            if self.logging:\n",
    "                if set_log_name:\n",
    "                    log_name=str(input('Enter log name'))\n",
    "                    if log_name=='':\n",
    "                        sys.exit()\n",
    "                else:\n",
    "                    log_name=None\n",
    "                run=wandb.init(project='AVR',name=log_name)\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                for batch_idx,batch in enumerate(self.train_dataloader):\n",
    "                    if n_batches=='full':\n",
    "                        pass\n",
    "                    if batch_idx==n_batches:\n",
    "                        break\n",
    "                    x,y=batch\n",
    "                    y_hat=self.forward(x)\n",
    "\n",
    "                    crossent_loss,reg_loss,loss,acc=self.calculate_loss(y_hat,y)\n",
    "                    loss.backward()\n",
    "                    self.optimiser.step()\n",
    "\n",
    "                    if self.logging:\n",
    "                        wandb.log({'epoch':epoch,\n",
    "                                    'train_loss':loss.item(),\n",
    "                                    'train_crossent_loss':crossent_loss.item(),\n",
    "                                    'train_reg_loss':reg_loss.item(),\n",
    "                                    'train_accuracy':acc\n",
    "                                    },step=self.glob_step)\n",
    "\n",
    "                    if (self.glob_step%val_every_n_steps==0) and (self.glob_step!=0):\n",
    "                        self.validation()\n",
    "\n",
    "                    self.glob_step+=1\n",
    "\n",
    "                #run ablation every n epochs\n",
    "                if (epoch%eval_every==0) and (epoch!=0):\n",
    "                    self.eval(self.test_dataloader1,self.test_dataloader2,n_batches=n_eval_batches)\n",
    "\n",
    "                    \n",
    "                print(f'Epoch: {epoch}, Loss:{loss.item()}')\n",
    "                self.train_epoch+=1\n",
    "            \n",
    "            wandb.finish()\n",
    "            print('Training finished')\n",
    "                \n",
    "    def validation(self):\n",
    "        batch=next(iter(self.test_dataloader1))\n",
    "        x,y=batch\n",
    "        with torch.no_grad():\n",
    "            y_hat=self.forward(x)\n",
    "        crossent_loss,reg_loss,loss,acc=self.calculate_loss(y_hat,y)\n",
    "\n",
    "        if self.logging:\n",
    "                wandb.log({\n",
    "                            'validation_loss':loss.item(),\n",
    "                            'validation_crossent_loss':crossent_loss.item(),\n",
    "                            'validation_reg_loss':reg_loss.item(),\n",
    "                            'validation_accuracy':acc\n",
    "                            },step=self.glob_step)\n",
    "\n",
    "    def eval(self,task_eval_dataloader,_task_eval_dataloader,n_batches):\n",
    "        '''\n",
    "        Evaluated mask via ablation\n",
    "\n",
    "        Ablation - frozen_parameters * ~binaries (inverted mask)\n",
    "        '''\n",
    "        #create masked model\n",
    "\n",
    "        acc1s=[]\n",
    "        acc2s=[]\n",
    "\n",
    "\n",
    "        for batch_idx,(batch1,batch2) in enumerate(zip(task_eval_dataloader,_task_eval_dataloader)):\n",
    "            if n_batches=='full':\n",
    "                pass\n",
    "            if batch_idx==n_batches:\n",
    "                break\n",
    "            x1,y1=batch1\n",
    "            x2,y2=batch2\n",
    "\n",
    "\n",
    "            pred_logits_1=self.forward(x1,invert_mask=True)\n",
    "            pred_logits_2=self.forward(x2,invert_mask=True)\n",
    "\n",
    "            acc1s.append(utils.calculate_accuracy(pred_logits_1,y1))\n",
    "            acc2s.append(utils.calculate_accuracy(pred_logits_2,y2))\n",
    "\n",
    "\n",
    "\n",
    "        acc1=round(np.mean(acc1s),2)\n",
    "        acc2=round(np.mean(acc2s),2)\n",
    "\n",
    "        if self.logging:\n",
    "            wandb.define_metric(\"Eval accuracies\",step_metric='epoch')\n",
    "            wandb.log({'Eval accuracies':{\"Task\":acc1,\"NOT task\":acc2}},step=self.glob_step)\n",
    "        else:\n",
    "            print({\"Acc task'\":acc1,\"Acc not task\":acc2})\n",
    "\n",
    "\n",
    "    def MaskedLinear(self,x,name,invert=False):\n",
    "\n",
    "        '''\n",
    "        Think invert detaches tensor from comp graph, so should only be used during val\n",
    "        '''\n",
    "        binaries=self.transform_logit_tensors() #we could just update binaries every training step\n",
    "        binary_weight,binary_bias=binaries[name+'.weight'],binaries[name+'.bias']\n",
    "        if invert:\n",
    "            binary_weight=(~(binary_weight.bool())).int()\n",
    "            binary_bias=(~(binary_bias.bool())).int()\n",
    "\n",
    "        masked_weight,masked_bias=self.param_dict[name+'.weight']*binary_weight,self.param_dict[name+'.bias']*binary_bias\n",
    "        out=F.linear(x,weight=masked_weight,bias=masked_bias)\n",
    "        return out\n",
    "\n",
    "    def MaskedConv2d(self,x,name,bias=False,invert=False):\n",
    "\n",
    "        '''\n",
    "        invert detaches tensor from comp graph, so should only be used during val\n",
    "        '''\n",
    "\n",
    "        stride,padding=self.leaf_modules[name].stride,self.leaf_modules[name].padding\n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "        binary_weight=binaries[name+'.weight']\n",
    "        masked_weight=self.param_dict[name+'.weight']*binary_weight\n",
    "\n",
    "        if bias:\n",
    "            binary_bias=binaries[name+'.bias']\n",
    "            masked_bias=self.param_dict[name+'bias']*binary_bias\n",
    "        else:\n",
    "            masked_bias=None\n",
    "\n",
    "        if invert:\n",
    "            binary_weight=(~(binary_weight.bool())).int()\n",
    "            if bias:\n",
    "                binary_bias=(~(binary_bias.bool())).int()\n",
    "\n",
    "        out=F.conv2d(x,weight=masked_weight,bias=masked_bias,stride=stride,padding=padding)\n",
    "        return out\n",
    "\n",
    "    def transform_logit_tensors(self):\n",
    "\n",
    "        tau=self.tau\n",
    "\n",
    "        U1 = torch.rand(1, requires_grad=True)\n",
    "        U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "        samples={}\n",
    "        for k,v in self.logit_tensors_dict.items():\n",
    "            samples[k]=torch.sigmoid((v - torch.log(torch.log(U1) / torch.log(U2))) / tau)\n",
    "            \n",
    "\n",
    "        binaries_stop={}\n",
    "        for k,v in samples.items():\n",
    "            with torch.no_grad():\n",
    "                binaries_stop[k]=(v>0.5).float()-v\n",
    "        \n",
    "        binaries={}\n",
    "        for k,v in binaries_stop.items():\n",
    "            binaries[k]=v+samples[k]\n",
    "\n",
    "        return binaries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaskedMNISTFFN(AbstractMaskedModel):\n",
    "\n",
    "    def __init__(self,kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #none mask trainable layers\n",
    "        self.layer0=nn.Flatten(start_dim=1,end_dim=-1)\n",
    "\n",
    "    def forward(self, x, invert_mask=False):\n",
    "        \n",
    "        \n",
    "        x0=self.layer0(x)\n",
    "        x1=self.MaskedLinear(x0,name='layers.1',invert=invert_mask)\n",
    "        x2=self.MaskedLinear(x1,name='layers.2',invert=invert_mask)\n",
    "        x3=self.MaskedLinear(x2,name='layers.3',invert=invert_mask)\n",
    "        x4=self.MaskedLinear(x3,name='layers.4',invert=invert_mask)\n",
    "\n",
    "        return x4\n",
    "\n",
    "\n",
    "\n",
    "class MaskedMNISTConv(AbstractMaskedModel):\n",
    "    \n",
    "    def __init__(self,kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "        #initialise layers that mask not trained on\n",
    "        #should implement method to check if we've done this right\n",
    "        self.maxpool_2=nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2_drop=nn.Dropout()\n",
    "\n",
    "    def forward(self,x,invert_mask=False):\n",
    "\n",
    "        N=x.size()[0]\n",
    "        \n",
    "        x=F.relu(self.maxpool_2(self.MaskedConv2d(x,name='conv1',invert=invert_mask)))\n",
    "        x=F.relu(self.maxpool_2(self.conv2_drop(self.MaskedConv2d(x,name='conv2',invert=invert_mask))))\n",
    "        x=x.view(N,-1)\n",
    "        x=F.relu(self.MaskedLinear(x,name='fc1',invert=invert_mask))\n",
    "        x=F.dropout(x)\n",
    "        x=self.MaskedLinear(x,name='fc2',invert=invert_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#REMEMBER TO RELOAD CELL ABOVE IF CHANGING AMM class\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "model=MNISTModel.load_from_checkpoint('AVR/test_l/checkpoints/epoch=9-step=9380.ckpt',img_size=(28,28))\n",
    "\n",
    "kwargs={\n",
    "    'model':model,\n",
    "    'train_dataloader':custom1_train_dataloader,\n",
    "    'test_dataloader1':custom1_test_dataloader,\n",
    "    'test_dataloader2':custom2_test_dataloader,\n",
    "    \n",
    "}\n",
    "\n",
    "train_kwargs={\n",
    "    'alpha':1e-5,\n",
    "    'n_epochs':5,\n",
    "    'n_batches':5,\n",
    "    'val_every_n_steps':10,\n",
    "    'eval_every':5,\n",
    "    'n_eval_batches':1,\n",
    "    'norm_freq':100,\n",
    "    'logging':False,\n",
    "    'set_log_name':True\n",
    "}\n",
    "\n",
    "mm1=MaskedMNISTConv(kwargs)\n",
    "mm1.train(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test=False\n",
    "\n",
    "def sweep_function(test=test):\n",
    "\n",
    "    run=wandb.init(project='AVR')\n",
    "\n",
    "    \n",
    "    model=MNISTModel.load_from_checkpoint('AVR/test_l/checkpoints/epoch=9-step=9380.ckpt',img_size=(28,28))\n",
    "    task_train_dataloader=custom1_train_dataloader\n",
    "    task_test_dataloader=custom1_test_dataloader\n",
    "    _task_test_dataloader=custom2_test_dataloader\n",
    "\n",
    "    alpha=wandb.config.alpha\n",
    "    n_epochs=wandb.config.n_epochs\n",
    "\n",
    "    mm_kwargs={\n",
    "        'model':model,\n",
    "        'train_dataloader':task_train_dataloader,\n",
    "        'test_dataloader1':task_test_dataloader,\n",
    "        'test_dataloader2':_task_test_dataloader\n",
    "    }\n",
    "\n",
    "    train_kwargs={\n",
    "    'n_batches':5 if test else 'full',\n",
    "    'val_every_n_steps':10,\n",
    "    'eval_every':5,\n",
    "    'n_eval_batches':2,\n",
    "    'norm_freq':100,\n",
    "    'logging':True,\n",
    "    }\n",
    "\n",
    "    masked_model=MaskedMNISTConv(mm_kwargs)\n",
    "    masked_model.train(alpha=alpha,n_epochs=n_epochs,**train_kwargs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration={\n",
    "    'method':'grid',\n",
    "    'name':str(input('Enter sweep name')),\n",
    "    'metric':{\n",
    "        'goal':'maximize',\n",
    "        'name':'validation_accuracy',\n",
    "        },\n",
    "    'parameters':{\n",
    "        'alpha':{'values':[1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1e0]},\n",
    "        'n_epochs':{'value':2 if test else 10}\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id=wandb.sweep(sweep=sweep_configuration,project='AVR')\n",
    "wandb.agent(sweep_id,function=sweep_function)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DLenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09911b70b107ce1f1a26d3d965c92acabc3f780c628bdef8c12485070fed524b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
