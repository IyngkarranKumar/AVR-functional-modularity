{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl;\n",
    "import importlib\n",
    "import numpy as np\n",
    "import data\n",
    "import copy\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from torch.special import logit\n",
    "importlib.reload(data)\n",
    "\n",
    "pl.seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Gumbel_Sigmoid(tens,T=1):\n",
    "    log_U1=torch.log(torch.rand_like(tens))\n",
    "    log_U2=torch.log(torch.rand_like(tens))\n",
    "    t1=(tens-torch.log(log_U1/log_U2))/T\n",
    "    t2=torch.sigmoid(t1)\n",
    "    return t2\n",
    "\n",
    "def indicator(tens,threshold=0.5,below=0,above=1):\n",
    "\n",
    "    t1=-1*F.threshold(tens,threshold=threshold,value=below)\n",
    "    t2=F.threshold(t1,threshold=-0.00001,value=above).int()\n",
    "    return t2\n",
    "\n",
    "def calculate_accuracy(pred_logits,true_idxs):\n",
    "\n",
    "    probs=F.softmax(pred_logits,dim=1)\n",
    "    pred=torch.argmax(probs,axis=1)\n",
    "    acc=(len(torch.nonzero(pred==true_idxs))/len(pred))*100\n",
    "    return acc\n",
    "    \n",
    "\n",
    "class Indicator(nn.Module):\n",
    "\n",
    "    def __init__(self,threshold=0.5,below=0,above=1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.l1=torch.nn.Threshold(threshold=threshold,value=below)\n",
    "        self.l2=torch.nn.Threshold(threshold=-1e-10,value=above)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.l2(-1*self.l1(x))\n",
    "        x=x.int()\n",
    "        return x\n",
    "\n",
    "class MaskedModel(nn.Module):\n",
    "\n",
    "    def __init__(self,model):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.masked_model=deepcopy(model)\n",
    "        self.indicator=Indicator()\n",
    "        self.trained_weights=list(model.parameters())\n",
    "        for w in self.trained_weights: w.requires_grad=False #model params are frozen\n",
    "        self.logit_mask=[nn.Parameter(torch.rand_like(w,requires_grad=True)) for w in self.trained_weights]\n",
    "        self.binarised_mask=[self.indicator(tens) for tens in self.logit_mask]\n",
    "\n",
    "\n",
    "\n",
    "        for i,param in enumerate(self.masked_model.parameters()):\n",
    "            param.data=self.trained_weights[i]*self.binarised_mask[i] #change weights of model\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        logits=self.masked_model(x)\n",
    "        return logits\n",
    "        \n",
    "\n",
    "    def logit_l2_loss(self,mode='mean'):\n",
    "        l2=0\n",
    "        for tens in self.logit_mask:\n",
    "            if mode=='mean':\n",
    "                l2+=(tens**2).mean()\n",
    "            elif mode=='sum':\n",
    "                l2+=(tens**2).sum()\n",
    "            else:\n",
    "                raise Exception(f'{mode} is an invalid l2 mode')\n",
    "\n",
    "        return l2\n",
    "\n",
    "    def mask_sparsity(self):\n",
    "        binarised_mask=self.binarised_mask\n",
    "        binarised_mask=[self.indicator(tens) for tens in self.logit_mask]\n",
    "        numel=sum(torch.numel(btens) for btens in binarised_mask)\n",
    "        num_ones=sum([torch.count_nonzero(btens).item() for btens in binarised_mask])\n",
    "        return ((num_ones/numel)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "epochs=2\n",
    "subset_frac=0.1\n",
    "\n",
    "\n",
    "data_path='datasets'\n",
    "\n",
    "train_dataset = MNIST(data_path,train=True, transform=transforms.ToTensor())\n",
    "idxs=np.random.choice(range(train_dataset.__len__()),int(train_dataset.__len__()*subset_frac),replace=False)\n",
    "subset=torch.utils.data.Subset(train_dataset,idxs)\n",
    "train_dataloader = DataLoader(subset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "val_dataset=MNIST(data_path,train=False,transform=transforms.ToTensor())\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "custom_dataset=data.MNISTCustomDataset(n=5,transform=transforms.ToTensor())\n",
    "idxs=np.random.choice(range(custom_dataset.__len__()),int(custom_dataset.__len__()*subset_frac),replace=False)\n",
    "custom_subset=torch.utils.data.Subset(custom_dataset,idxs)\n",
    "custom_trainloader=DataLoader(custom_subset,batch_size=batch_size)\n",
    "\n",
    "X,y=next(iter(train_dataloader))\n",
    "X_c,y_c=next(iter(custom_trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(28*28,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.l1(x)\n",
    "        return y\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.maxpool2(self.conv1(x)))\n",
    "        x=F.relu(self.maxpool2(self.conv2_drop(self.conv2(x)))) #dropout in conv layers\n",
    "        x=x.view(-1,320)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.dropout(x,training=self.training) #dropout in FFN layers\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        print(f\"accuracy: {acc}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n",
    "debug=True\n",
    "if debug==True:\n",
    "    limit_train_batches=0.05\n",
    "    limit_val_batches=0.01\n",
    "else:\n",
    "    limit_train_batches=1.0\n",
    "    limit_val_batches=1.0\n",
    "\n",
    "epochs=3\n",
    "model=MNISTModel()\n",
    "trainer=pl.Trainer(max_epochs=3,\n",
    "                    check_val_every_n_epoch=1,\n",
    "                    limit_train_batches=limit_train_batches,limit_val_batches=limit_val_batches)\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maskedmodel=MaskedModel(model)\n",
    "\n",
    "\n",
    "epochs=3\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimiser=torch.optim.Adam(maskedmodel.parameters())\n",
    "optimiser.add_param_group({'params':maskedmodel.logit_mask})\n",
    "\n",
    "print('Training masked model')\n",
    "for _ in range(epochs):\n",
    "    running_loss=0\n",
    "    for i,batch in enumerate(custom_trainloader):\n",
    "        print(f'Step {i} of epoch {_}')\n",
    "        optimiser.zero_grad()\n",
    "        X,y=batch\n",
    "        logits=maskedmodel(X)\n",
    "\n",
    "        sum1=sum(t1.sum() for t1 in maskedmodel.logit_mask)\n",
    "\n",
    "        l2_loss=maskedmodel.logit_l2_loss()\n",
    "        CE_loss=criterion(logits,y)\n",
    "        loss=CE_loss+l2_loss; running_loss+=loss\n",
    "        running_loss+=loss\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        sum2=sum(t1.sum() for t1 in maskedmodel.logit_mask)\n",
    "\n",
    "        #print(f\"Delta logit mask {np.abs((sum1-sum2).item())}\")\n",
    "        \n",
    "    running_loss/=i #mean loss over epoch\n",
    "    print(f\"\\n Average epoch loss: {running_loss}\")\n",
    "    print(f'Non-zero proportion {maskedmodel.mask_sparsity()} \\n')\n",
    "\n",
    "print('\\n Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ablation study\n",
    "logit_mask=maskedmodel.logit_mask\n",
    "binarised_mask=[indicator(tens) for tens in logit_mask]\n",
    "model2=copy.deepcopy(model) #we need to check if masking has changed the original model weights\n",
    "\n",
    "model2.eval()\n",
    "\n",
    "\n",
    "for idx,param in enumerate(model2.parameters()):\n",
    "    bool_idxs=binarised_mask[idx]==1 #the 'on' weights in mask\n",
    "    param.data[bool_idxs]=0 #zero ablating 'on weights'\n",
    "\n",
    "\n",
    "\n",
    "#calculate accuracy of model 2\n",
    "\n",
    "accs=[]\n",
    "for idx,batch in enumerate(custom_trainloader):\n",
    "    X,y=batch\n",
    "    pred_logits=model2(X)\n",
    "    acc=calculate_accuracy(pred_logits=pred_logits,true_idxs=y)\n",
    "    accs.append(acc)\n",
    "\n",
    "avg_acc=sum(accs)/len(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DLenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09911b70b107ce1f1a26d3d965c92acabc3f780c628bdef8c12485070fed524b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
