{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl;\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import data\n",
    "import utils\n",
    "import sys\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from copy import deepcopy\n",
    "from torch.special import logit\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "importlib.reload(data)\n",
    "importlib.reload(utils)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "debug=False\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Gumbel_Sigmoid(tens,T=1):\n",
    "    log_U1=torch.log(torch.rand_like(tens))\n",
    "    log_U2=torch.log(torch.rand_like(tens))\n",
    "    t1=(tens-torch.log(log_U1/log_U2))/T\n",
    "    t2=torch.sigmoid(t1)\n",
    "    return t2\n",
    "\n",
    "def calculate_accuracy(pred_logits,true_idxs):\n",
    "\n",
    "    probs=F.softmax(pred_logits,dim=1)\n",
    "    pred=torch.argmax(probs,axis=1)\n",
    "    acc=(len(torch.nonzero(pred==true_idxs))/len(pred))*100\n",
    "    return acc\n",
    "\n",
    "\n",
    "def indicator(tens,threshold=0.5,below=0,above=1):\n",
    "    #allow it, it works\n",
    "\n",
    "    t1=F.threshold(-1*tens,threshold=-1*threshold,value=above)\n",
    "    t2=F.threshold(t1,threshold=0,value=below)\n",
    "    \n",
    "    return t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m importlib\u001b[39m.\u001b[39mreload(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importlib' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(data)\n",
    "\n",
    "batch_size=64\n",
    "epochs=2\n",
    "subset_frac=0.1\n",
    "\n",
    "custom_dataset_test_split=0.2\n",
    "\n",
    "data_path='datasets'\n",
    "\n",
    "train_dataset = MNIST(data_path,train=True, transform=transforms.ToTensor())\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "\n",
    "val_dataset=MNIST(data_path,train=False,transform=transforms.ToTensor())\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size)\n",
    "\n",
    "if not debug:\n",
    "    custom1_datamodule=data.CustomDataModule(n=5)\n",
    "    custom1_datamodule.setup()\n",
    "    custom1_train_dataloader=custom1_datamodule.train_dataloader()\n",
    "    custom1_test_dataloader=custom1_datamodule.test_dataloader()\n",
    "\n",
    "    custom2_datamodule=data.CustomDataModule(n=9)\n",
    "    custom2_datamodule.setup()\n",
    "    custom2_train_dataloader=custom2_datamodule.train_dataloader()\n",
    "    custom2_test_dataloader=custom2_datamodule.test_dataloader()\n",
    "\n",
    "if debug:\n",
    "    custom1_datamodule=data.CustomDataModule(n=5,dataset_frac=subset_frac)\n",
    "    custom1_datamodule.setup()\n",
    "    custom1_train_dataloader=custom1_datamodule.train_dataloader()\n",
    "    custom1_test_dataloader=custom1_datamodule.test_dataloader()\n",
    "\n",
    "    custom2_datamodule=data.CustomDataModule(n=9,dataset_frac=subset_frac)\n",
    "    custom2_datamodule.setup()\n",
    "    custom2_train_dataloader=custom2_datamodule.train_dataloader()\n",
    "    custom2_test_dataloader=custom2_datamodule.test_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "X,y=next(iter(train_dataloader))\n",
    "X_c,y_c=next(iter(custom1_train_dataloader))\n",
    "X_c2,y_c2=next(iter(custom2_train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl_loggers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#regular MNIST Model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tb_logger\u001b[39m=\u001b[39mpl_loggers\u001b[39m.\u001b[39mTensorBoardLogger(save_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./runs/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSimpleModel\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/masking.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl_loggers' is not defined"
     ]
    }
   ],
   "source": [
    "#regular MNIST Model\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(28*28,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.l1(x)\n",
    "        return y\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.maxpool2(self.conv1(x)))\n",
    "        x=F.relu(self.maxpool2(self.conv2_drop(self.conv2(x)))) #dropout in conv layers\n",
    "        x=x.view(-1,320)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.dropout(x,training=self.training) #dropout in FFN layers\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        self.log('train loss',loss.item(),on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        \n",
    "        self.log('val loss',loss.item(),on_epoch=True)\n",
    "        self.log('val acc',acc,on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n",
    "MNISTConvModel=MNISTModel()\n",
    "\n",
    "class MNISTFFN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Flatten(start_dim=1,end_dim=-1),\n",
    "            nn.Linear(28*28,256),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Linear(128,64),\n",
    "            nn.Linear(64,10),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "\n",
    "        #logging\n",
    "        self.logger.experiment.add_scalars('Pretrained loss',{'train':loss.item()},self.global_step)\n",
    "        self.logger.experiment.add_scalars('Pretrained accuracy',{'train':acc},self.global_step)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        \n",
    "        #logging\n",
    "        self.logger.experiment.add_scalars('Pretrained loss',{'val':loss.item()},self.global_step)\n",
    "        self.logger.experiment.add_scalars('Pretrained accuracy',{'val':acc},self.global_step)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ablation study\n",
    "logit_mask=maskedmodel.logit_mask\n",
    "binarised_mask=[indicator(tens) for tens in logit_mask]\n",
    "model2=copy.deepcopy(model) #we need to check if masking has changed the original model weights\n",
    "\n",
    "model2.eval()\n",
    "\n",
    "#ablate\n",
    "for idx,param in enumerate(model2.parameters()):\n",
    "    bool_idxs=binarised_mask[idx]==1 #the 'on' weights in mask\n",
    "    param.data[bool_idxs]=0 #zero ablating 'on weights'\n",
    "\n",
    "\n",
    "\n",
    "#calculate accuracy of model 2 on custom dataset\n",
    "accs_c1=[]\n",
    "for idx,batch in enumerate(custom1_test_dataloader):\n",
    "    X,y=batch\n",
    "    pred_logits=model2(X)\n",
    "    acc=calculate_accuracy(pred_logits=pred_logits,true_idxs=y)\n",
    "    accs_c1.append(acc)\n",
    "\n",
    "avg_acc_c1=sum(accs_c1)/len(accs_c1)\n",
    "\n",
    "accs_c2=[]\n",
    "for idx,batch in enumerate(custom2_test_dataloader):\n",
    "    X,y=batch\n",
    "    pred_logits=model2(X)\n",
    "    acc=calculate_accuracy(pred_logits=pred_logits,true_idxs=y)\n",
    "    accs_c2.append(acc)\n",
    "\n",
    "avg_acc_c2=sum(accs_c2)/len(accs_c2)\n",
    "\n",
    "print(f'Ablated model accuracy on C1: {avg_acc_c1} \\\n",
    "    Ablated model accuracy on C2: {avg_acc_c2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretraining model - SHOULDN't BE RUNNING OFTEN\n",
    "\n",
    "run_bool=input(\"Are you sure you want to retrain model? yes/no\")\n",
    "\n",
    "if run_bool=='yes':\n",
    "    pass\n",
    "elif run_bool=='no':\n",
    "    sys.exit()\n",
    "else:\n",
    "    raise Exception('Please enter either yes or no')\n",
    "\n",
    "\n",
    "\n",
    "tb_logger=pl_loggers.TensorBoardLogger(save_dir='logs') #for Lightning\n",
    "\n",
    "if debug==True:\n",
    "    limit_train_batches=0.05\n",
    "    limit_val_batches=0.01\n",
    "else:\n",
    "    limit_train_batches=1.0\n",
    "    limit_val_batches=1.0\n",
    "\n",
    "check_val_every=1\n",
    "\n",
    "epochs=5\n",
    "model=MNISTFFN()\n",
    "trainer=pl.Trainer(max_epochs=epochs,\n",
    "                    check_val_every_n_epoch=check_val_every,\n",
    "                    limit_train_batches=limit_train_batches,limit_val_batches=limit_val_batches,\n",
    "                    logger=tb_logger)\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying from https://discuss.pytorch.org/t/how-to-optimise-mask-on-a-fully-frozen-network/151438\n",
    "\n",
    "model=MNISTFFN().load_from_checkpoint('logs/lightning_logs/version_7/checkpoints/epoch=4-step=4690.ckpt')\n",
    "\n",
    "log_name='Mask alpha-10e-5'\n",
    "log_dir=os.path.join('logs',log_name)\n",
    "writer=SummaryWriter(log_dir=log_dir) #for PyTorch\n",
    "\n",
    "for layer in model.layers:\n",
    "    if len(list(layer.parameters()))!=0: #if has trainable parameter\n",
    "        layer.weight.requires_grad=False\n",
    "\n",
    "\n",
    "'''logit initilisation'''\n",
    "logits=[]\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer,nn.Linear):\n",
    "        logits.append(torch.nn.Parameter(data=torch.full_like(layer.weight.clone(),0.9),requires_grad=True))\n",
    "        logits.append(torch.nn.Parameter(data=torch.full_like(layer.bias.clone(),0.9),requires_grad=True))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(logits, lr=0.01)\n",
    "\n",
    "'''Initialise hyper-parameters'''\n",
    "NUM_EPOCHS = 30 # NB: check for number of training epochs in paper\n",
    "n_batches=5\n",
    "tau = 1  # temperature parameter, NB: check for value in paper\n",
    "alpha = 0.005\n",
    "\n",
    "step=0\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print(f'Epoch: {e}')\n",
    "    for batch_idx,batch in enumerate(custom1_train_dataloader):\n",
    "        \n",
    "\n",
    "        #limited training for debugging\n",
    "        if batch_idx==30:\n",
    "            break\n",
    "        X,y=batch\n",
    "\n",
    "\n",
    "        #mask transform (logits->bin)\n",
    "\n",
    "        U1 = torch.rand(1, requires_grad=True)\n",
    "        U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "        samples = []\n",
    "        for layer in logits:       \n",
    "            samples.append(torch.sigmoid((layer - torch.log(torch.log(U1) / torch.log(U2))) / tau))\n",
    "\n",
    "        #Ensures that the binarisation step has no associated gradient\n",
    "        binaries_stop = []        \n",
    "        for layer in samples:\n",
    "            with torch.no_grad():\n",
    "                binaries_stop.append((layer > 0.5).float() - layer)\n",
    "\n",
    "        binaries = []\n",
    "        for idx, layer in enumerate(binaries_stop):\n",
    "            binaries.append(layer+samples[idx])\n",
    "\n",
    "        binaries_iter=iter(binaries)\n",
    "\n",
    "        #manual inference\n",
    "        for idx,layer in enumerate(model.layers):\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                weight=layer.weight*next(binaries_iter) #manual application of mask\n",
    "                bias=layer.bias*next(binaries_iter)\n",
    "                X=F.linear(X,weight,bias)\n",
    "            else:\n",
    "                X=layer(X)\n",
    "            pred_logits=X\n",
    "\n",
    "        reg_loss = torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in logits]))\n",
    "        loss=criterion(pred_logits,y)+alpha*reg_loss\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        #logging\n",
    "        writer.add_scalar('Mask loss',loss.item(),step)\n",
    "        writer.add_scalars('A+S',\n",
    "                           {'accuracy '+log_name:utils.calculate_accuracy(pred_logits,y),\n",
    "                            'sparsity '+log_name:utils.sparsity(binaries)},\n",
    "                           step)\n",
    "\n",
    "\n",
    "        step+=1\n",
    "\n",
    "    \n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelA=MNISTFFN().load_from_checkpoint('logs/lightning_logs/version_7/checkpoints/epoch=4-step=4690.ckpt')\n",
    "modelB=MNISTModel() #with conv enc layer\n",
    "\n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return model\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "def get_named_children(model):\n",
    "\n",
    "    '''\n",
    "    IMPORTANT: We assume that a leaf child is one that has 0 children\n",
    "    This needs checking\n",
    "    '''\n",
    "    \n",
    "    children_dict={}\n",
    "    named_modules=dict(model.named_modules())\n",
    "    for module_name,module in named_modules.items():\n",
    "        if len(list(module.children()))==0:\n",
    "            children_dict[module_name]=module\n",
    "\n",
    "    return children_dict\n",
    "\n",
    "class MaskedModel():\n",
    "\n",
    "    def __init__(self,model,train_dataloader,test_dataloader1,test_dataloader2,alpha=0.4,tau=1,eval_freq=5):\n",
    "\n",
    "        self.model=model\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.test_dataloader1=test_dataloader1\n",
    "        self.test_dataloader2=test_dataloader2\n",
    "\n",
    "        self.logit_tensors_dict={k:torch.nn.Parameter(data=torch.full_like(p,0.9)) for k,p in modelA.named_parameters()}\n",
    "        self.alpha=alpha\n",
    "        self.tau=tau\n",
    "        self.logging=False\n",
    "\n",
    "        self.eval_freq=eval_freq\n",
    "\n",
    "        #freeze model parameters\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad=False\n",
    "\n",
    "        self.leaf_modules=get_named_children(self.model)\n",
    "\n",
    "\n",
    "        self.optimiser=torch.optim.Adam(self.logit_tensors_dict.values())\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "\n",
    "        for layer_name,layer in self.leaf_modules.items():\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                x=F.linear(x,weight=binaries[layer_name+'.weight'],bias=binaries[layer_name+'.bias']) #calling next rather than idx-ing ensures tensor not detached from computational graph (what does this mean)\n",
    "            else:\n",
    "                x=layer(x)\n",
    "\n",
    "        return x\n",
    "        #we must implement the forward layer ourself\n",
    "\n",
    "        '''\n",
    "        for layer_name,layer in self.layers:\n",
    "            if linear:\n",
    "                x=F.linear(x,weight=binaries[layer_name+'weight'],bias=binaries[layer_name+'bias'])\n",
    "        '''\n",
    "        \n",
    "    \n",
    "       \n",
    "\n",
    "    def train(self,n_batches=10,n_epochs=5,logging=False,n_eval_batches=5):\n",
    "\n",
    "        if logging:\n",
    "            self.logging=True\n",
    "        if self.logging:\n",
    "            log_name=str(input('Enter log name'))\n",
    "            wandb.init(project='AVR',name=log_name)\n",
    "\n",
    "        for ep in range(n_epochs):\n",
    "            for batch_idx,batch in enumerate(train_dataloader):\n",
    "                if n_batches=='full':\n",
    "                    continue\n",
    "                if batch_idx==n_batches:\n",
    "                    break\n",
    "                x,y=batch\n",
    "                y_hat=self.forward(x)\n",
    "\n",
    "                crossent_loss=F.cross_entropy(y_hat,y)\n",
    "                reg_loss=torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "                loss=crossent_loss+self.alpha*reg_loss\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "\n",
    "                if self.logging:\n",
    "                    wandb.log({'epoch':ep,\n",
    "                                'loss':loss,\n",
    "                                'crossent_loss':crossent_loss,\n",
    "                                'reg_loss':reg_loss,\n",
    "                                })\n",
    "\n",
    "            if (ep%self.eval_freq==0) and (ep!=0):\n",
    "                self.eval(self.test_dataloader1,self.test_dataloader2,n_batches=n_eval_batches)\n",
    "        \n",
    "        if self.logging:\n",
    "            wandb.finish()\n",
    "\n",
    "    \n",
    "    def transform_logit_tensors(self):\n",
    "\n",
    "        tau=self.tau\n",
    "\n",
    "        U1 = torch.rand(1, requires_grad=True)\n",
    "        U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "        samples={}\n",
    "        for k,v in self.logit_tensors_dict.items():\n",
    "            samples[k]=torch.sigmoid((v - torch.log(torch.log(U1) / torch.log(U2))) / tau)\n",
    "            \n",
    "\n",
    "        binaries_stop={}\n",
    "        for k,v in samples.items():\n",
    "            with torch.no_grad():\n",
    "                binaries_stop[k]=(v>0.5).float()-v\n",
    "        \n",
    "        binaries={}\n",
    "        for k,v in binaries_stop.items():\n",
    "            binaries[k]=v+samples[k]\n",
    "\n",
    "        return binaries\n",
    "\n",
    "    def eval(self,task_eval_dataloader,_task_eval_dataloader,n_batches='full'):\n",
    "        #evaluate mask via ablation\n",
    "        print('start eval')\n",
    "\n",
    "        #create masked model\n",
    "        masked_model=copy.deepcopy(self.model)\n",
    "        with torch.no_grad():\n",
    "            binaries=self.transform_logit_tensors()\n",
    "            #only linear layer compatibility at the moment \n",
    "            for n,p in masked_model.named_parameters():\n",
    "                masked_param=p*binaries[n]\n",
    "                p.copy_(masked_param) #copy in masked params\n",
    "\n",
    "\n",
    "        acc1s=[]\n",
    "        acc2s=[]\n",
    "\n",
    "        for batch_idx,(batch1,batch2) in enumerate(zip(task_eval_dataloader,_task_eval_dataloader)):\n",
    "            if n_batches=='full':\n",
    "                continue\n",
    "            if batch_idx==n_batches:\n",
    "                break\n",
    "            x1,y1=batch1\n",
    "            x2,y2=batch2\n",
    "\n",
    "            pred_logits_1=masked_model(x1)\n",
    "            pred_logits_2=masked_model(x2)\n",
    "\n",
    "            acc1s.append(utils.calculate_accuracy(pred_logits_1,y1))\n",
    "            acc2s.append(utils.calculate_accuracy(pred_logits_2,y2))\n",
    "\n",
    "        acc1=round(np.mean(acc1s),2)\n",
    "        acc2=round(np.mean(acc2s),2)\n",
    "\n",
    "        if self.logging:\n",
    "            wandb.log({\"Acc task'\":acc1,\"Acc not task\":acc2})\n",
    "\n",
    "        print('end eval')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "\n",
    "\n",
    "mm=MaskedModel(modelA,train_dataloader=task_train_dataloader,\n",
    "    test_dataloader1=task_test_dataloader,test_dataloader2=_task_test_dataloader)\n",
    "mm.train(n_batches=10,n_epochs=10,logging=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ablation study\n",
    "\n",
    "#transform trained mask logits\n",
    "\n",
    "U1 = torch.rand(1, requires_grad=True)\n",
    "U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "samples = []\n",
    "for layer in logits:       \n",
    "    samples.append(torch.sigmoid((layer - torch.log(torch.log(U1) / torch.log(U2))) / tau))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "binaries_stop = []        \n",
    "for layer in samples:\n",
    "    with torch.no_grad():\n",
    "        binaries_stop.append((layer > 0.5).float() - layer)\n",
    "\n",
    "binaries = []\n",
    "for idx, layer in enumerate(binaries_stop):\n",
    "    binaries.append(layer+samples[idx])\n",
    "\n",
    "binaries_iter=iter(binaries)\n",
    "\n",
    "#replace tens\n",
    "masked_model=copy.deepcopy(model)\n",
    "with torch.no_grad():\n",
    "    for p in masked_model.parameters():\n",
    "        masked_tens=p*next(binaries_iter)\n",
    "        p.copy_(masked_tens)\n",
    "\n",
    "\n",
    "\n",
    "acc1s=[]\n",
    "acc2s=[]\n",
    "#eval on trained dataset\n",
    "\n",
    "for batch1,batch2 in zip(custom1_test_dataloader,custom2_test_dataloader):\n",
    "    x1,y1=batch1\n",
    "    x2,y2=batch2\n",
    "\n",
    "    pred_logits_1=masked_model(x1)\n",
    "    pred_logits_2=masked_model(x2)\n",
    "\n",
    "    acc1s.append(utils.calculate_accuracy(pred_logits_1,y1))\n",
    "    acc2s.append(utils.calculate_accuracy(pred_logits_2,y2))\n",
    "\n",
    "acc1=round(np.mean(acc1s),2)\n",
    "acc2=round(np.mean(acc2s),2)\n",
    "\n",
    "print(f'Acc 1 - {acc1}, \\n Acc 2 - {acc2}')\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DLenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09911b70b107ce1f1a26d3d965c92acabc3f780c628bdef8c12485070fed524b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
