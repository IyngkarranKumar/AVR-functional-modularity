{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl;\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import data\n",
    "import utils\n",
    "import sys\n",
    "import wandb\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from abc import ABC,abstractmethod\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from copy import deepcopy\n",
    "from torch.special import logit\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "importlib.reload(data)\n",
    "importlib.reload(utils)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "debug=False\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "\n",
    "batch_size=128\n",
    "epochs=2\n",
    "subset_frac=0.1\n",
    "\n",
    "custom_dataset_test_split=0.2\n",
    "\n",
    "data_path='datasets'\n",
    "\n",
    "train_dataset = MNIST(data_path,train=True, transform=transforms.ToTensor())\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "\n",
    "val_dataset=MNIST(data_path,train=False,transform=transforms.ToTensor())\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size)\n",
    "\n",
    "n_task=5\n",
    "n_task_=[1,2,3,4,6,7,8,9]\n",
    "\n",
    "if not debug:\n",
    "    custom1_datamodule=data.CustomDataModule(n=n_task)\n",
    "    custom1_datamodule.setup()\n",
    "    custom1_train_dataloader=custom1_datamodule.train_dataloader()\n",
    "    custom1_test_dataloader=custom1_datamodule.test_dataloader()\n",
    "\n",
    "    custom2_datamodule=data.CustomDataModule(n=n_task_)\n",
    "    custom2_datamodule.setup()\n",
    "    custom2_train_dataloader=custom2_datamodule.train_dataloader()\n",
    "    custom2_test_dataloader=custom2_datamodule.test_dataloader()\n",
    "\n",
    "if debug:\n",
    "    custom1_datamodule=data.CustomDataModule(n=n_task,dataset_frac=subset_frac)\n",
    "    custom1_datamodule.setup()\n",
    "    custom1_train_dataloader=custom1_datamodule.train_dataloader()\n",
    "    custom1_test_dataloader=custom1_datamodule.test_dataloader()\n",
    "\n",
    "    custom2_datamodule=data.CustomDataModule(n=n_task_,dataset_frac=subset_frac)\n",
    "    custom2_datamodule.setup()\n",
    "    custom2_train_dataloader=custom2_datamodule.train_dataloader()\n",
    "    custom2_test_dataloader=custom2_datamodule.test_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "X,y=next(iter(train_dataloader))\n",
    "X_c,y_c=next(iter(custom1_train_dataloader))\n",
    "X_c2,y_c2=next(iter(custom2_train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular MNIST Model\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(28*28,10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.l1(x)\n",
    "        return y\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,img_size):\n",
    "        super().__init__()\n",
    "        H,W=img_size\n",
    "        nf1=10\n",
    "        nf2=20\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,nf1,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2 = nn.Conv2d(10,nf2,kernel_size=3,stride=1,padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        lin_size=int(0.25*H*0.25*W*nf2) #must be integer\n",
    "        self.fc1 = nn.Linear(in_features=lin_size,out_features=50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        N=x.size()[0]\n",
    "\n",
    "        x=F.relu(self.maxpool2(self.conv1(x)))\n",
    "        x=F.relu(self.maxpool2(self.conv2_drop(self.conv2(x)))) #dropout in conv layers\n",
    "        x=x.view(N,-1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.dropout(x,training=self.training) #dropout in FFN layers\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        self.log('train-loss',loss.item(),on_epoch=True)\n",
    "        #wandb.log('train/loss':loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        \n",
    "        self.log('val-loss',loss.item(),on_epoch=True)\n",
    "        self.log('val-acc',acc,on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n",
    "\n",
    "class MNISTFFN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Flatten(start_dim=1,end_dim=-1),\n",
    "            nn.Linear(28*28,256),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Linear(128,64),\n",
    "            nn.Linear(64,10),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y=batch\n",
    "        y_hat=self(x)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "\n",
    "        #logging\n",
    "        self.log('train-loss',loss.item())\n",
    "        self.log('train-acc',acc)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx,on_epoch=True):\n",
    "        X,y=batch\n",
    "        y_hat=self(X)\n",
    "        loss=F.cross_entropy(y_hat,y)\n",
    "        probs=F.softmax(y_hat,dim=1)\n",
    "        pred=torch.argmax(probs,axis=1)\n",
    "        acc=(len(torch.nonzero(pred==y))/len(pred))*100\n",
    "        \n",
    "        #logging\n",
    "        self.log('val-loss',loss.item())\n",
    "        self.log('val-acc',acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretraining model - SHOULDN't BE RUNNING OFTEN\n",
    "\n",
    "\n",
    "checkpoint_dir='checkpoints'\n",
    "\n",
    "name=str(input('Log name'))\n",
    "wandb_logger=WandbLogger(project='AVR',name=name,version=name)\n",
    "\n",
    "if debug==True:\n",
    "\n",
    "    limit_train_batches=0.05\n",
    "    limit_val_batches=0.01\n",
    "    \n",
    "else:\n",
    "    limit_train_batches=1.0\n",
    "    limit_val_batches=1.0\n",
    "\n",
    "check_val_every=1\n",
    "\n",
    "epochs=10\n",
    "model=MNISTFFN()\n",
    "\n",
    "#callbacks\n",
    "ES_callback=EarlyStopping(monitor='val-loss',patience=100)\n",
    "ckpt_callback=ModelCheckpoint(\n",
    "    save_top_k=1, #save top 1 checkpoint,\n",
    "    monitor='val-acc',\n",
    "    mode='max',\n",
    "    filename='max-val-acc-ckpt'\n",
    ")\n",
    "\n",
    "\n",
    "trainer=pl.Trainer(max_epochs=epochs,\n",
    "                    check_val_every_n_epoch=check_val_every,\n",
    "                    limit_train_batches=limit_train_batches,limit_val_batches=limit_val_batches,\n",
    "                    logger=wandb_logger,callbacks=[ES_callback],\n",
    "                    default_root_dir=checkpoint_dir)\n",
    "\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelA=MNISTFFN().load_from_checkpoint('logs/lightning_logs/version_7/checkpoints/epoch=4-step=4690.ckpt')\n",
    "modelB=MNISTModel(img_size=(28,28)) #with conv enc layer\n",
    "\n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return model\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "\n",
    "\n",
    "def get_named_children(model):\n",
    "\n",
    "    '''\n",
    "    IMPORTANT: We assume that a leaf child is one that has 0 children\n",
    "    This needs checking\n",
    "    '''\n",
    "    \n",
    "    children_dict={}\n",
    "    named_modules=dict(model.named_modules())\n",
    "    for module_name,module in named_modules.items():\n",
    "        if len(list(module.children()))==0:\n",
    "            children_dict[module_name]=module\n",
    "\n",
    "    return children_dict\n",
    "\n",
    "class MaskedModel():\n",
    "\n",
    "    def __init__(self,model,train_dataloader,test_dataloader1,test_dataloader2,tau=1):\n",
    "\n",
    "        self.model=model\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.test_dataloader1=test_dataloader1\n",
    "        self.test_dataloader2=test_dataloader2\n",
    "\n",
    "        self.logit_tensors_dict={k:torch.nn.Parameter(data=torch.full_like(p,0.9)) for k,p in modelA.named_parameters()}\n",
    "        self.alpha=None\n",
    "        self.tau=tau\n",
    "        self.logging=False\n",
    "\n",
    "        self.train_epoch=0\n",
    "\n",
    "\n",
    "        #freeze model parameters\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad=False\n",
    "        self.param_dict=dict(model.named_parameters())\n",
    "\n",
    "        self.leaf_modules=get_named_children(self.model)\n",
    "\n",
    "\n",
    "        self.optimiser=torch.optim.Adam(self.logit_tensors_dict.values())\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        '''\n",
    "        Forward through masked model \n",
    "        Masked model - Frozen pretrained model * binaries\n",
    "        '''\n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "\n",
    "        #apply mask to (frozen) tensors\n",
    "        for layer_name,layer in self.leaf_modules.items():\n",
    "            if isinstance(layer,nn.Linear):\n",
    "                weight_=self.param_dict[layer_name+'.weight']*binaries[layer_name+'.weight']\n",
    "                bias_=self.param_dict[layer_name+'.bias']*binaries[layer_name+'.bias']\n",
    "                x=F.linear(x,weight=weight_,bias=bias_) #calling next rather than idx-ing ensures tensor not detached from computational graph (what does this mean)\n",
    "            else:\n",
    "                x=layer(x)\n",
    "\n",
    "        return x\n",
    "        #we must implement the forward layer ourself\n",
    "\n",
    "        '''\n",
    "        for layer_name,layer in self.layers:\n",
    "            if linear:\n",
    "                x=F.linear(x,weight=binaries[layer_name+'weight'],bias=binaries[layer_name+'bias'])\n",
    "        '''\n",
    "        \n",
    "    \n",
    "       \n",
    "\n",
    "    def train(self,alpha,n_batches=10,n_epochs=5,logging=False,\n",
    "            val_every_n_steps=10,\n",
    "            eval_every=10,n_eval_batches=5,norm_freq=5,set_log_name=False):\n",
    "\n",
    "        if logging:\n",
    "            self.logging=True\n",
    "        if self.logging:\n",
    "            if set_log_name:\n",
    "                log_name=str(input('Enter log name'))\n",
    "                if log_name=='':\n",
    "                    sys.exit()\n",
    "            else:\n",
    "                log_name=None\n",
    "            run=wandb.init(project='AVR',name=log_name)\n",
    "\n",
    "        for ep in range(n_epochs):\n",
    "\n",
    "            #set class attributes to be used in other methods called in 'train()'\n",
    "            self.train_epoch+=1\n",
    "            self.alpha=alpha\n",
    "\n",
    "\n",
    "            for batch_idx,batch in enumerate(train_dataloader):\n",
    "                if n_batches=='full':\n",
    "                    pass\n",
    "                if batch_idx==n_batches:\n",
    "                    break\n",
    "                x,y=batch\n",
    "                y_hat=self.forward(x)\n",
    "\n",
    "                crossent_loss=F.cross_entropy(y_hat,y)\n",
    "                reg_loss=alpha*torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "                loss=crossent_loss+reg_loss\n",
    "                acc=utils.calculate_accuracy(y_hat,y)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "\n",
    "                if self.logging:\n",
    "                    wandb.log({'epoch':ep,\n",
    "                                'train_loss':loss.item(),\n",
    "                                'train_crossent_loss':crossent_loss.item(),\n",
    "                                'train_reg_loss':reg_loss.item(),\n",
    "                                'train_accuracy':acc\n",
    "                                })\n",
    "                                \n",
    "                    if ep%norm_freq==0 and ep!=0:\n",
    "                        data=self.param_grad_norms()\n",
    "                        table=wandb.Table(data=data,columns=['names','key','values'])\n",
    "                        wandb.log({'Norm data':table})\n",
    "\n",
    "                if (run.step%val_every_n_steps==0) and (run.step!=0):\n",
    "                    self.validation()\n",
    "\n",
    "\n",
    "            #evaluate via ablation and comparison to other tasks\n",
    "            if (ep%eval_every==0):\n",
    "                self.eval(self.test_dataloader1,self.test_dataloader2,n_batches=n_eval_batches)\n",
    "            \n",
    "            print(f'Epoch: {ep}, Loss:{loss.item()}')\n",
    "\n",
    "\n",
    "        \n",
    "        if self.logging:\n",
    "            wandb.finish()\n",
    "\n",
    "    \n",
    "    def transform_logit_tensors(self):\n",
    "\n",
    "        tau=self.tau\n",
    "\n",
    "        U1 = torch.rand(1, requires_grad=True)\n",
    "        U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "        samples={}\n",
    "        for k,v in self.logit_tensors_dict.items():\n",
    "            samples[k]=torch.sigmoid((v - torch.log(torch.log(U1) / torch.log(U2))) / tau)\n",
    "            \n",
    "\n",
    "        binaries_stop={}\n",
    "        for k,v in samples.items():\n",
    "            with torch.no_grad():\n",
    "                binaries_stop[k]=(v>0.5).float()-v\n",
    "        \n",
    "        binaries={}\n",
    "        for k,v in binaries_stop.items():\n",
    "            binaries[k]=v+samples[k]\n",
    "\n",
    "        return binaries\n",
    "\n",
    "\n",
    "    def validation(self):\n",
    "        '''\n",
    "        Calculate accuracy of mask on validation set\n",
    "        For now, we use test_dataset=val_dataset\n",
    "        '''\n",
    "        \n",
    "        batch=next(iter(self.test_dataloader1))\n",
    "        x,y=batch\n",
    "        with torch.no_grad():\n",
    "            y_hat=self.forward(x)\n",
    "\n",
    "        crossent_loss=F.cross_entropy(y_hat,y)\n",
    "        reg_loss=self.alpha*torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "        loss=crossent_loss+reg_loss\n",
    "        acc=utils.calculate_accuracy(y_hat,y)\n",
    "\n",
    "\n",
    "\n",
    "        if self.logging:\n",
    "                wandb.log({\n",
    "                            'validation_loss':loss.item(),\n",
    "                            'validation_crossent_loss':crossent_loss.item(),\n",
    "                            'validation_reg_loss':reg_loss.item(),\n",
    "                            'validation_accuracy':acc\n",
    "                            })\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self,task_eval_dataloader,_task_eval_dataloader,n_batches):\n",
    "        '''\n",
    "        Evaluated mask via ablation\n",
    "\n",
    "        Ablation - frozen_parameters * ~binaries (inverted mask)\n",
    "        '''\n",
    "        print('start eval')\n",
    "        #create masked model\n",
    "        masked_model=copy.deepcopy(self.model)\n",
    "        with torch.no_grad():\n",
    "            binaries=self.transform_logit_tensors()\n",
    "\n",
    "            #only linear layer compatibility at the moment \n",
    "            for n,p in masked_model.named_parameters():\n",
    "                invert_mask=(~(binaries[n].bool())).int() #to ablate TASK weights, we invert mask\n",
    "                masked_param=p*invert_mask\n",
    "                p.copy_(masked_param) #copy in masked params\n",
    "\n",
    "        acc1s=[]\n",
    "        acc2s=[]\n",
    "\n",
    "        for batch_idx,(batch1,batch2) in enumerate(zip(task_eval_dataloader,_task_eval_dataloader)):\n",
    "            if n_batches=='full':\n",
    "                pass\n",
    "            if batch_idx==n_batches:\n",
    "                break\n",
    "            x1,y1=batch1\n",
    "            x2,y2=batch2\n",
    "\n",
    "            pred_logits_1=masked_model(x1)\n",
    "            pred_logits_2=masked_model(x2)\n",
    "\n",
    "            acc1s.append(utils.calculate_accuracy(pred_logits_1,y1))\n",
    "            acc2s.append(utils.calculate_accuracy(pred_logits_2,y2))\n",
    "\n",
    "        acc1=round(np.mean(acc1s),2)\n",
    "        acc2=round(np.mean(acc2s),2)\n",
    "\n",
    "        if self.logging:\n",
    "            wandb.define_metric(\"Eval accuracies\",step_metric='epoch')\n",
    "            wandb.log({'Eval accuracies':{\"Task\":acc1,\"NOT task\":acc2}})\n",
    "        else:\n",
    "            print({\"Acc task'\":acc1,\"Acc not task\":acc2})\n",
    "\n",
    "        print('end eval')\n",
    "\n",
    "    def param_grad_norms(self):\n",
    "        data=[]\n",
    "        names=list(self.logit_tensors_dict.keys())\n",
    "        for name in names:\n",
    "            data.append([name,'param norm',self.logit_tensors_dict[name].mean().item()])\n",
    "            data.append([name,'grad norm',self.logit_tensors_dict[name].grad.mean().item()])\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "\n",
    "\n",
    "kwargs={\n",
    "    'alpha':1e-5,\n",
    "    'n_epochs':5,\n",
    "    'n_batches':10,\n",
    "    'val_every_n_steps':10,\n",
    "    'eval_every':2,\n",
    "    'n_eval_batches':1,\n",
    "    'norm_freq':100,\n",
    "    'logging':False,\n",
    "    'set_log_name':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if 0:\n",
    "    mm=MaskedModel(modelA,train_dataloader=task_train_dataloader,\n",
    "        test_dataloader1=task_test_dataloader,test_dataloader2=_task_test_dataloader)\n",
    "    mm.train(**kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMaskedModel(ABC):\n",
    "\n",
    "    def __init__(self,model,train_dataloader,test_dataloader1,test_dataloader2,savedir=None):\n",
    "        \n",
    "        self.model=model\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.test_dataloader1=test_dataloader1\n",
    "        self.test_dataloader2=test_dataloader2\n",
    "        #freeze model parameters\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad=False\n",
    "        self.param_dict=dict(model.named_parameters())\n",
    "        self.leaf_modules=utils.get_named_children(self.model)\n",
    "        self.savedir=savedir\n",
    "\n",
    "\n",
    "        self.logit_tensors_dict={k:torch.nn.Parameter(data=torch.full_like(p,0.9)) for k,p in model.named_parameters()}\n",
    "        self.alpha=None\n",
    "        self.logging=False #this attribute and below are set during training/loading\n",
    "        self.logger=None\n",
    "        self.run_id=None\n",
    "        self.optimiser=torch.optim.Adam(self.logit_tensors_dict.values())\n",
    "\n",
    "        self.global_step=0\n",
    "        self.train_epoch=0\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self,x,invert_mask=False):\n",
    "        pass\n",
    "\n",
    "    def calculate_loss(self,y_hat,y):\n",
    "        crossent_loss=F.cross_entropy(y_hat,y)\n",
    "        reg_loss=self.alpha*torch.sum(torch.stack([torch.sum(logit_tens) for logit_tens in list(self.logit_tensors_dict.values())]))\n",
    "        loss=crossent_loss+reg_loss\n",
    "        acc=utils.calculate_accuracy(y_hat,y)\n",
    "\n",
    "        return crossent_loss,reg_loss,loss,acc\n",
    "\n",
    "    def train(self,alpha,tau=1,n_epochs=5,n_batches=5,batch_split=4,\n",
    "                    val_every_n_steps=10,n_val_batches=100,\n",
    "                    eval_every=10,n_eval_batches=5,\n",
    "                    logging=False,set_log_name=False,save_freq=10):\n",
    "\n",
    "\n",
    "            #set class attributes for use in rest of class\n",
    "            self.alpha=alpha\n",
    "            self.tau=tau\n",
    "            \n",
    "            if logging:\n",
    "                self.logging=True\n",
    "            if self.logging:\n",
    "                if set_log_name:\n",
    "                    log_name=str(input('Enter log name'))\n",
    "                    if log_name=='':\n",
    "                        sys.exit()\n",
    "                else:\n",
    "                    log_name=None\n",
    "                \n",
    "                if self.run_id is not None:\n",
    "                    self.logger=wandb.init(id=self.run_id,project='AVR',resume='must')\n",
    "                else:\n",
    "                    self.logger=wandb.init(project='AVR',name=log_name)\n",
    "                wandb.define_metric('global step')\n",
    "\n",
    "            for epoch in range(self.train_epoch,n_epochs):\n",
    "                start_time=timer()\n",
    "                for batch_idx,batch in enumerate(self.train_dataloader):\n",
    "                    if n_batches=='full':\n",
    "                        pass\n",
    "                    if batch_idx==n_batches:\n",
    "                        break\n",
    "\n",
    "\n",
    "                    train_loss=0\n",
    "                    split_X,split_y=torch.chunk(batch[0],batch_split),torch.chunk(batch[1],batch_split)\n",
    "                    for x,y in zip(split_X,split_y):\n",
    "                        y_hat=self.forward(x)\n",
    "                        crossent_loss,reg_loss,loss,acc=self.calculate_loss(y_hat,y)\n",
    "                        train_loss+=loss.item()\n",
    "                        loss.backward()\n",
    "\n",
    "                    self.optimiser.step()\n",
    "\n",
    "                    if self.logging:\n",
    "                        '''\n",
    "                        wandb.log({'epoch':epoch,\n",
    "                                    'train_loss':train_loss,\n",
    "                                    },step=self.global_step)\n",
    "                        '''\n",
    "                        wandb.define_metric('train_loss',step_metric='global_step')\n",
    "                        wandb.log({'train_loss':train_loss,'global_step':self.global_step})\n",
    "\n",
    "                    if (self.global_step%val_every_n_steps==0) and (self.global_step!=0):\n",
    "                        self.validation(n_batches=n_val_batches)\n",
    "\n",
    "                    self.global_step+=1\n",
    "                    \n",
    "\n",
    "                end_train_time=timer()\n",
    "\n",
    "\n",
    "\n",
    "                #run ablation every n_ablation epochs\n",
    "                if (epoch%eval_every==0) and (epoch!=0):\n",
    "                    self.eval(self.test_dataloader1,self.test_dataloader2,n_batches=n_eval_batches)\n",
    "                    end_eval_time=timer()\n",
    "\n",
    "                    train_time=end_train_time-start_time\n",
    "                    eval_time=end_eval_time-end_train_time\n",
    "                    print(f'Train time: {train_time} \\n Eval time:{eval_time}')\n",
    "\n",
    "                \n",
    "                #save every n_save epochs\n",
    "                if (self.savedir is not None) and (epoch%save_freq==0) and (epoch!=0):\n",
    "                    self.save()\n",
    "\n",
    "                    \n",
    "                print(f'Epoch: {epoch}, Loss:{loss.item()}')\n",
    "                self.train_epoch+=1\n",
    "                \n",
    "            \n",
    "            wandb.finish()\n",
    "            print('Training finished')\n",
    "                \n",
    "    def validation(self,n_batches):\n",
    "        batch=next(iter(self.test_dataloader1))\n",
    "\n",
    "        losses=[]\n",
    "        val_accs=[]\n",
    "\n",
    "        for batch_idx,batch in enumerate(self.test_dataloader1):\n",
    "            if n_batches=='full':\n",
    "                pass\n",
    "            if batch_idx==n_batches:\n",
    "                break\n",
    "\n",
    "            x,y=batch\n",
    "            with torch.no_grad():\n",
    "                y_hat=self.forward(x)\n",
    "            crossent_loss,reg_loss,loss,acc=self.calculate_loss(y_hat,y)\n",
    "            losses.append((crossent_loss.item(),reg_loss.item(),loss.item()))\n",
    "            val_accs.append(acc)\n",
    "\n",
    "        val_crossent_loss=np.mean([_[0] for _ in losses])\n",
    "        val_reg_loss=np.mean([_[1] for _ in losses])\n",
    "        val_loss=np.mean([_[2] for _ in losses])\n",
    "        val_accuracy=np.mean(val_accs)\n",
    "\n",
    "        if self.logging:\n",
    "                wandb.log({\n",
    "                            'validation_loss':val_loss,\n",
    "                            'validation_crossent_loss':val_crossent_loss,\n",
    "                            'validation_reg_loss':val_reg_loss,\n",
    "                            'validation_accuracy':val_accuracy\n",
    "                            })\n",
    "        else:\n",
    "            #print(f'\\n Validation accuracy: {acc}')\n",
    "            pass\n",
    "\n",
    "        \n",
    "\n",
    "    def eval(self,task_eval_dataloader,_task_eval_dataloader,n_batches):\n",
    "        '''\n",
    "        Evaluated mask via ablation\n",
    "\n",
    "        Ablation - frozen_parameters * ~binaries (inverted mask)\n",
    "        '''\n",
    "        #create masked model\n",
    "\n",
    "        acc1s=[]\n",
    "        acc2s=[]\n",
    "\n",
    "\n",
    "        for batch_idx,(batch1,batch2) in enumerate(zip(task_eval_dataloader,_task_eval_dataloader)):\n",
    "            if n_batches=='full':\n",
    "                pass\n",
    "            if batch_idx==n_batches:\n",
    "                break\n",
    "            x1,y1=batch1\n",
    "            x2,y2=batch2\n",
    "\n",
    "\n",
    "            pred_logits_1=self.forward(x1,invert_mask=True)\n",
    "            pred_logits_2=self.forward(x2,invert_mask=True)\n",
    "\n",
    "            acc1s.append(utils.calculate_accuracy(pred_logits_1,y1))\n",
    "            acc2s.append(utils.calculate_accuracy(pred_logits_2,y2))\n",
    "\n",
    "\n",
    "\n",
    "        acc1=round(np.mean(acc1s),2)\n",
    "        acc2=round(np.mean(acc2s),2)\n",
    "\n",
    "        if self.logging:\n",
    "            wandb.define_metric(\"Eval accuracies\",step_metric='epoch')\n",
    "            wandb.log({'Eval accuracies':{\"Task\":acc1,\"NOT task\":acc2}},step=self.global_step)\n",
    "        else:\n",
    "            print({\"Acc task'\":acc1,\"Acc not task\":acc2})\n",
    "\n",
    "\n",
    "    def MaskedLinear(self,x,name,invert=False):\n",
    "\n",
    "        '''\n",
    "        Think invert detaches tensor from comp graph, so should only be used during val\n",
    "        '''\n",
    "        binaries=self.transform_logit_tensors() #we could just update binaries every training step\n",
    "        binary_weight,binary_bias=binaries[name+'.weight'],binaries[name+'.bias']\n",
    "        if invert:\n",
    "            binary_weight=(~(binary_weight.bool())).int()\n",
    "            binary_bias=(~(binary_bias.bool())).int()\n",
    "\n",
    "            '''\n",
    "            idxs0_w,idxs1_w=binary_weight==0.0,binary_weight==1.0\n",
    "            idxs0_b,idxs1_b=binary_bias==0.0,binary_bias==0.0\n",
    "            binary_weight[idxs0_w]+=1.0\n",
    "            binary_weight[idxs1_w]-=-1.0\n",
    "            binary_bias[idxs0_b]=+1.0\n",
    "            binary_bias[idxs1_b]-=1.0\n",
    "            '''\n",
    "\n",
    "        masked_weight,masked_bias=self.param_dict[name+'.weight']*binary_weight,self.param_dict[name+'.bias']*binary_bias\n",
    "        out=F.linear(x,weight=masked_weight,bias=masked_bias)\n",
    "        return out\n",
    "\n",
    "    def MaskedConv2d(self,x,name,bias=False,invert=False):\n",
    "\n",
    "        '''\n",
    "        invert detaches tensor from comp graph, so should only be used during val\n",
    "        '''\n",
    "\n",
    "        stride,padding=self.leaf_modules[name].stride,self.leaf_modules[name].padding\n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "        binary_weight=binaries[name+'.weight']\n",
    "\n",
    "        if bias:\n",
    "            binary_bias=binaries[name+'.bias']\n",
    "        else:\n",
    "            masked_bias=None\n",
    "\n",
    "        if invert:\n",
    "            binary_weight=(~(binary_weight.bool())).int()\n",
    "            if bias:\n",
    "                binary_bias=(~(binary_bias.bool())).int()\n",
    "\n",
    "        masked_weight=self.param_dict[name+'.weight']*binary_weight\n",
    "        masked_bias=self.param_dict[name+'.bias']*binary_bias\n",
    "        out=F.conv2d(x,weight=masked_weight,bias=masked_bias,stride=stride,padding=padding)\n",
    "        return out\n",
    "\n",
    "    def MaskedBatchNorm2d(self,x,name,invert=False):\n",
    "        \n",
    "        #these are approximations to feature mean + variance over whole dataset, calculated during training\n",
    "        running_mean=self.leaf_modules[name].running_mean\n",
    "        running_var=self.leaf_modules[name].running_var \n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "        binary_weight=binaries[name+'.weight']\n",
    "        binary_bias=binaries[name+'.bias']\n",
    "\n",
    "        if invert:\n",
    "            binary_weight=(~(binary_weight.bool())).int()\n",
    "            binary_bias=(~(binary_bias.bool())).int()\n",
    "        \n",
    "        masked_weight=self.param_dict[name+'.weight']*binary_weight\n",
    "        masked_bias=self.param_dict[name+'.bias']*binary_bias\n",
    "        return F.batch_norm(x,running_mean=running_mean,running_var=running_var,weight=masked_weight,bias=binary_bias)\n",
    "\n",
    "    def MaskedLayerNorm(self,x,name,invert=False):\n",
    "\n",
    "        normalized_shape=self.leaf_modules[name].normalized_shape\n",
    "\n",
    "        binaries=self.transform_logit_tensors()\n",
    "        binary_weight=binaries[name+'.weight']\n",
    "        binary_bias=binaries[name+'.bias']\n",
    "\n",
    "        if invert:\n",
    "            binary_weight=(~(binary_weight.bool())).int()\n",
    "            binary_bias=(~(binary_bias.bool())).int()\n",
    "        \n",
    "        masked_weight=self.param_dict[name+'.weight']*binary_weight\n",
    "        masked_bias=self.param_dict[name+'.bias']*binary_bias\n",
    "\n",
    "        return F.layer_norm(normalized_shape=normalized_shape,weight=masked_weight,bias=masked_bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def transform_logit_tensors(self):\n",
    "\n",
    "        tau=self.tau\n",
    "\n",
    "        U1 = torch.rand(1, requires_grad=True)\n",
    "        U2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "        samples={}\n",
    "        for k,v in self.logit_tensors_dict.items():\n",
    "            samples[k]=torch.sigmoid((v - torch.log(torch.log(U1) / torch.log(U2))) / tau)\n",
    "            \n",
    "\n",
    "        binaries_stop={}\n",
    "        for k,v in samples.items():\n",
    "            with torch.no_grad():\n",
    "                binaries_stop[k]=(v>0.5).float()-v\n",
    "        \n",
    "        binaries={}\n",
    "        for k,v in binaries_stop.items():\n",
    "            binaries[k]=v+samples[k]\n",
    "\n",
    "        return binaries\n",
    "\n",
    "    def save(self):\n",
    "\n",
    "        if not os.path.isdir(self.savedir):\n",
    "            os.mkdir(self.savedir)\n",
    "\n",
    "        save_dict={}\n",
    "        save_dict['alpha']=self.alpha\n",
    "        save_dict['tau']=self.tau\n",
    "        save_dict['global_step']=self.global_step\n",
    "        save_dict['train_epoch']=self.train_epoch\n",
    "        save_dict['logit_tensors_dict']=self.logit_tensors_dict\n",
    "        save_dict['optimiser']=self.optimiser\n",
    "        if self.logging:\n",
    "            save_dict['run_id']=self.logger.id\n",
    "        else:\n",
    "            save_dict['run_id']=None\n",
    "\n",
    "        fname=os.path.join(self.savedir,f'checkpoint_step={self.global_step}_epoch={self.train_epoch}')\n",
    "        with open(fname,'wb') as f:\n",
    "            pickle.dump(save_dict,f)\n",
    "            print(f'Checkpoint step={self.global_step}, epoch={self.train_epoch} saved')\n",
    " \n",
    "    def load(self,path):\n",
    "\n",
    "        with open (path,'rb') as f:\n",
    "            load_dict=pickle.load(f)\n",
    "\n",
    "        self.alpha=load_dict.get('alpha')\n",
    "        self.tau=load_dict.get('tau')\n",
    "        self.global_step=load_dict.get('global_step')\n",
    "        self.train_epoch=load_dict.get('train_epoch')\n",
    "        self.logit_tensors_dict=load_dict.get('logit_tensors_dict')\n",
    "        self.optimiser=load_dict.get('optimiser')\n",
    "        self.run_id=load_dict.get('run_id')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MaskedMNISTFFN(AbstractMaskedModel):\n",
    "\n",
    "    def __init__(self,kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #none mask trainable layers\n",
    "        self.layer0=nn.Flatten(start_dim=1,end_dim=-1)\n",
    "\n",
    "    def forward(self, x, invert_mask=False):\n",
    "        \n",
    "        \n",
    "        x0=self.layer0(x)\n",
    "        x1=self.MaskedLinear(x0,name='layers.1',invert=invert_mask)\n",
    "        x2=self.MaskedLinear(x1,name='layers.2',invert=invert_mask)\n",
    "        x3=self.MaskedLinear(x2,name='layers.3',invert=invert_mask)\n",
    "        x4=self.MaskedLinear(x3,name='layers.4',invert=invert_mask)\n",
    "\n",
    "        return x4\n",
    "\n",
    "\n",
    "\n",
    "class MaskedMNISTConv(AbstractMaskedModel):\n",
    "    \n",
    "    def __init__(self,kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "        #initialise layers that mask not trained on\n",
    "        #should implement method to check if we've done this right\n",
    "        self.maxpool_2=nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2_drop=nn.Dropout()\n",
    "\n",
    "    def forward(self,x,invert_mask=False):\n",
    "\n",
    "        N=x.size()[0]\n",
    "        \n",
    "        x=F.relu(self.maxpool_2(self.MaskedConv2d(x,name='conv1',invert=invert_mask)))\n",
    "        x=F.relu(self.maxpool_2(self.conv2_drop(self.MaskedConv2d(x,name='conv2',invert=invert_mask))))\n",
    "        x=x.view(N,-1)\n",
    "        x=F.relu(self.MaskedLinear(x,name='fc1',invert=invert_mask))\n",
    "        x=F.dropout(x)\n",
    "        x=self.MaskedLinear(x,name='fc2',invert=invert_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskedSCLModel(AbstractMaskedModel):\n",
    "\n",
    "    def __init__(self,kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.flatten_layer=nn.Flatten(1)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "\n",
    "    def MaskedVisionNet(self,x,invert_mask=False):\n",
    "\n",
    "        vision_module_names=[_ for _ in self.leaf_modules.keys() if 'vision' in _]\n",
    "        vision_modules={k:self.leaf_modules[k] for k in vision_module_names}\n",
    "\n",
    "        for name,module in vision_modules.items():\n",
    "            if isinstance(module,nn.Conv2d):\n",
    "                x=self.MaskedConv2d(x,name=name,bias=True,invert=invert_mask)\n",
    "            elif isinstance(module,nn.Linear):\n",
    "                x=self.MaskedLinear(x,name=name,invert=invert_mask)\n",
    "            elif isinstance(module,nn.BatchNorm2d):\n",
    "                x=self.MaskedConv2d(x,name=name,invert=invert_mask)\n",
    "            elif isinstance(module,nn.LayerNorm):\n",
    "                pass\n",
    "            elif isinstance(module,nn.Flatten):\n",
    "                pass\n",
    "            elif isinstance(module,nn.ReLU):\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception('Unrecognised module')\n",
    "\n",
    "\n",
    "\n",
    "        x=self.MaskedConv2d(x,name='vision.net.0',bias=True)\n",
    "        x=self.MaskedBatchNorm2d(x,name='vision.net.1')\n",
    "\n",
    "        x=self.MaskedConv2d(x,name='vision.net.2',bias=True)\n",
    "        x=self.MaskedBatchNorm2d(x,name='vision.net.3',bias=True)\n",
    "\n",
    "        x=self.MaskedConv2d(x,name='vision.net.4',bias=True)\n",
    "        x=self.MaskedBatchNorm2d(x,name='vision.net.5',bias=True)\n",
    "\n",
    "        x=self.MaskedConv2d(x,name='vision.net.6',bias=True)\n",
    "        x=self.MaskedBatchNorm2d(x,name='vision.net.7',bias=True)\n",
    "\n",
    "        x_conv_out=self.MaskedConv2d(x,name='vision.net.8',bias=True)\n",
    "\n",
    "        x1=self.flatten_layer(x_conv_out)\n",
    "        x1=self.MaskedLinear(x1,name='vision.net.10')\n",
    "        x1=self.relu(x1)\n",
    "\n",
    "        #feedforward residual layer\n",
    "        x2=self.MaskedLinear(x1,name='vision.net.12.net.0')\n",
    "        x2=self.MaskedLayerNorm(x2,name='vision.net.12.net.1')\n",
    "        x2=self.relu(x2)\n",
    "        x2=self.MaskedLinear(x2,name='vision.net.12.net.3')\n",
    "        out=x2+x1\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def MaskedAttrNet(self,x,invert_mask=False):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, invert_mask=False):\n",
    "\n",
    "        b,m,n,c,h,w=x.shape\n",
    "        images=x.view(-1,c,h,w)\n",
    "\n",
    "\n",
    "        features=self.MaskedVision(x,invert_mask=invert_mask)\n",
    "        attrs=self.MaskedAttrNet(x,invert_mask=invert_mask)\n",
    "        attrs=self.MaskedFFResidual(x,invert_mask=invert_mask)\n",
    "\n",
    "        rels=self.MaskedRelNet(attrs,invert_mask=invert_mask)\n",
    "        rels=rels.flatten(2)\n",
    "\n",
    "        logits=self.MaskedToLogit(rels,invert_mask=invert_mask).flatten(1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "from scattering_transform import SCLTrainingWrapper,SCL\n",
    "\n",
    "scl_kwargs={\n",
    "    \"image_size\":160,                            # size of image\n",
    "    \"set_size\": 9,                               # number of questions + 1 answer\n",
    "    \"conv_channels\": [1, 16, 16, 32, 32, 32],    # convolutional channel progression, 1 for greyscale, 3 for rgb\n",
    "    \"conv_output_dim\": 80,                       # model dimension, the output dimension of the vision net\n",
    "    \"attr_heads\": 10,                            # number of attribute heads\n",
    "    \"attr_net_hidden_dims\": [128],               # attribute scatter transform MLP hidden dimension(s)\n",
    "    \"rel_heads\": 80,                             # number of relationship heads\n",
    "    \"rel_net_hidden_dims\": [64, 23, 5] \n",
    "}\n",
    "\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "model=SCLTrainingWrapper(SCL(**scl_kwargs))\n",
    "\n",
    "kwargs={\n",
    "    'model':model,\n",
    "    'train_dataloader':custom1_train_dataloader,\n",
    "    'test_dataloader1':custom1_test_dataloader,\n",
    "    'test_dataloader2':custom2_test_dataloader,\n",
    "    'savedir':'model_ckpts/FFN'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from scattering_transform import SCLTrainingWrapper, SCL\n",
    "SCL_model=SCLTrainingWrapper(SCL(**scl_kwargs))\n",
    "masked_scl=MaskedSCLModel(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#REMEMBER TO RELOAD CELL ABOVE IF CHANGING AMM class\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "model=MNISTFFN.load_from_checkpoint('/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/model_ckpts/MNISTFFNepoch=9-step=9380.ckpt')\n",
    "\n",
    "kwargs={\n",
    "    'model':model,\n",
    "    'train_dataloader':custom1_train_dataloader,\n",
    "    'test_dataloader1':custom1_test_dataloader,\n",
    "    'test_dataloader2':custom2_test_dataloader,\n",
    "    'savedir':'model_ckpts/FFN'\n",
    "}\n",
    "\n",
    "train_kwargs={\n",
    "    'alpha':1e-5,\n",
    "    'n_epochs':10,\n",
    "    'n_batches':1,\n",
    "    'val_every_n_steps':100,\n",
    "    'n_val_batches':1,\n",
    "    'eval_every':1,\n",
    "    'n_eval_batches':2,\n",
    "    'logging':True,\n",
    "    'set_log_name':True,\n",
    "    'batch_split':10,\n",
    "    'save_freq':4\n",
    "\n",
    "}\n",
    "\n",
    "mm1=MaskedMNISTFFN(kwargs)\n",
    "mm1.load('/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/model_ckpts/FFN/checkpoint_step=5_epoch=4')\n",
    "\n",
    "if 0:\n",
    "    mm1.train(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test=False\n",
    "\n",
    "def sweep_function(test=test,model_type='FFN'):\n",
    "\n",
    "    run=wandb.init(project='AVR')\n",
    "\n",
    "    \n",
    "    model=MNISTFFN().load_from_checkpoint('/Users/iyngkarrankumar/Documents/AI/AVR-functional-modularity/model_ckpts/MNISTFFNepoch=9-step=9380.ckpt')\n",
    "    task_train_dataloader=custom1_train_dataloader\n",
    "    task_test_dataloader=custom1_test_dataloader\n",
    "    _task_test_dataloader=custom2_test_dataloader\n",
    "\n",
    "    alpha=wandb.config.alpha\n",
    "    n_epochs=wandb.config.n_epochs\n",
    "\n",
    "    mm_kwargs={\n",
    "        'model':model,\n",
    "        'train_dataloader':task_train_dataloader,\n",
    "        'test_dataloader1':task_test_dataloader,\n",
    "        'test_dataloader2':_task_test_dataloader\n",
    "    }\n",
    "\n",
    "    train_kwargs={\n",
    "    'n_batches':5 if test else 'full',\n",
    "    'val_every_n_steps':10,\n",
    "    'eval_every':2,\n",
    "    'n_eval_batches':100,\n",
    "    'norm_freq':100,\n",
    "    'logging':True,\n",
    "    }\n",
    "    \n",
    "    if model_type=='FFN':\n",
    "        print(f'Model type:{model_type}')\n",
    "        masked_model=MaskedMNISTFFN(mm_kwargs)\n",
    "    elif model_type=='Conv':\n",
    "        print(f'Model type:{model_type}')\n",
    "        masked_model=MaskedMNISTConv(mm_kwargs)\n",
    "    else:\n",
    "        raise Exception('Enter valid model type')\n",
    "    masked_model.train(alpha=alpha,n_epochs=n_epochs,**train_kwargs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration={\n",
    "    'method':'grid',\n",
    "    \n",
    "    'name':str(input('Enter sweep name')),\n",
    "    'metric':{\n",
    "        'goal':'maximize',\n",
    "        'name':'validation_accuracy',\n",
    "        },\n",
    "    'parameters':{\n",
    "        'alpha':{'values':[1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1e0]},\n",
    "        'n_epochs':{'value':2 if test else 10}\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id=wandb.sweep(sweep=sweep_configuration,project='AVR')\n",
    "wandb.agent(sweep_id,function=sweep_function)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scl\n",
    "import scattering_transform\n",
    "from scattering_transform import SCLTrainingWrapper\n",
    "import utils\n",
    "import torch\n",
    "\n",
    "\n",
    "scl_kwargs={\n",
    "    \"image_size\":160,                            # size of image\n",
    "    \"set_size\": 9,                               # number of questions + 1 answer\n",
    "    \"conv_channels\": [1, 16, 16, 32, 32, 32],    # convolutional channel progression, 1 for greyscale, 3 for rgb\n",
    "    \"conv_output_dim\": 80,                       # model dimension, the output dimension of the vision net\n",
    "    \"attr_heads\": 10,                            # number of attribute heads\n",
    "    \"attr_net_hidden_dims\": [128],               # attribute scatter transform MLP hidden dimension(s)\n",
    "    \"rel_heads\": 80,                             # number of relationship heads\n",
    "    \"rel_net_hidden_dims\": [64, 23, 5] \n",
    "}\n",
    "\n",
    "SCL_model=scattering_transform.SCL(**scl_kwargs)\n",
    "\n",
    "#REMEMBER TO RELOAD CELL ABOVE IF CHANGING AMM class\n",
    "\n",
    "task_train_dataloader=custom1_train_dataloader\n",
    "task_test_dataloader=custom1_test_dataloader\n",
    "_task_test_dataloader=custom2_test_dataloader\n",
    "\n",
    "kwargs={\n",
    "    'model':SCL_model,\n",
    "    'train_dataloader':custom1_train_dataloader,\n",
    "    'test_dataloader1':custom1_test_dataloader,\n",
    "    'test_dataloader2':custom2_test_dataloader,\n",
    "    'savedir':'model_ckpts/FFN'\n",
    "}\n",
    "\n",
    "train_kwargs={\n",
    "    'alpha':1e-5,\n",
    "    'n_epochs':10,\n",
    "    'n_batches':1,\n",
    "    'val_every_n_steps':100,\n",
    "    'n_val_batches':1,\n",
    "    'eval_every':1,\n",
    "    'n_eval_batches':2,\n",
    "    'logging':True,\n",
    "    'set_log_name':True,\n",
    "    'batch_split':10,\n",
    "    'save_freq':4\n",
    "\n",
    "}\n",
    "\n",
    "mm1=MaskedSCLModel(kwargs)\n",
    "\n",
    "\n",
    "\n",
    "if 0:\n",
    "    mm1.train(**train_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DLenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09911b70b107ce1f1a26d3d965c92acabc3f780c628bdef8c12485070fed524b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
